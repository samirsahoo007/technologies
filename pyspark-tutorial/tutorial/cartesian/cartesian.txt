# ./pyspark
Python 2.6.9 (unknown, Sep  9 2014, 15:05:12)
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/

Using Python version 2.6.9 (unknown, Sep  9 2014 15:05:12)
SparkContext available as sc, SQLContext available as sqlCtx.
>>> a = [('k1','v1'), ('k2', 'v2')]
>>> a
[('k1', 'v1'), ('k2', 'v2')]
>>> b = [('k3','v3'), ('k4', 'v4'), ('k5', 'v5') ]
>>> b
[('k3', 'v3'), ('k4', 'v4'), ('k5', 'v5')]
>>> rdd1= sc.parallelize(a)
>>> rdd1.collect()
[('k1', 'v1'), ('k2', 'v2')]
>>> rdd2= sc.parallelize(b)
>>> rdd2.collect()
[('k3', 'v3'), ('k4', 'v4'), ('k5', 'v5')]
>>> rdd3 = rdd1.cartesian(rdd2)
>>> rdd3.collect()
[
 (('k1', 'v1'), ('k3', 'v3')), 
 (('k1', 'v1'), ('k4', 'v4')), 
 (('k1', 'v1'), ('k5', 'v5')), 
 (('k2', 'v2'), ('k3', 'v3')), 
 (('k2', 'v2'), ('k4', 'v4')), 
 (('k2', 'v2'), ('k5', 'v5'))
]
>>>
