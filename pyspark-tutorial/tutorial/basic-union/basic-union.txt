# ./pyspark
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.4.0
      /_/

Using Python version 2.6.9 (unknown, Sep  9 2014 15:05:12)
SparkContext available as sc, SQLContext available as sqlContext.

>>> d1= [('k1', 1), ('k2', 2), ('k3', 5)]
>>> d1
[('k1', 1), ('k2', 2), ('k3', 5)]

>>> d2= [('k1', 3), ('k2',4), ('k4', 8)]
>>> d2
[('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd1 = sc.parallelize(d1)
>>> rdd1.collect()
[('k1', 1), ('k2', 2), ('k3', 5)]

>>> rdd2 = sc.parallelize(d2)
>>> rdd2.collect();
[('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd3 = rdd1.union(rdd2)
>>> rdd3.collect()
[('k1', 1), ('k2', 2), ('k3', 5), ('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd4 = rdd3.reduceByKey(lambda x,y: x+y)
>>> rdd4.collect()
[('k3', 5), ('k2', 6), ('k1', 4), ('k4', 8)]