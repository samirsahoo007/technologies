# ./pyspark
Python 2.6.9 (unknown, Sep  9 2014, 15:05:12)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.0
      /_/

Using Python version 2.6.9 (unknown, Sep  9 2014 15:05:12)
SparkContext available as sc.
>>> sc
<pyspark.context.SparkContext object at 0x10ab3e210>
>>>
>>> nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 20])
>>> nums.collect()
[1, 2, 3, 4, 5, 6, 7, 8, 20]
>>> sumAndCount = nums.map(lambda x: (x, 1)).fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
>>> sumAndCount
(56, 9)
>>>
>>> avg = float(sumAndCount[0]) / float(sumAndCount[1])
>>> avg
6.2222222222222223
>>>
