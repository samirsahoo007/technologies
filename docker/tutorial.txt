COMMONLY USED COMMANDS:
======================
docker login docker.xyzcompany.com
docker pull docker.xyzcompany.com/protonbot/projecty-x:version-1.2 (FIRST TIME ONLY)
docker images											  # Get image id
docker run -v /Users/samirsahoo/Projects/:/opt/data -v /tmp/:/tmp -it 535022205989 /bin/bash      # Where 535022205989 is Image id
												  Mounts HOST directory(/Users/samirsahoo/Projects/:/opt/data) to IMAGE(/opt/data) directory and starts CONTAINER 																# and falls to bash shell of the contailer
												  # docker run -it <image_id> /bin/bash: Run this command if you do not want to mount anything
												  # Run the following command before running nosetests

												  # echo "export PYTHONPATH=$PATHONPATH:/opt/data/mnnny-tests:/opt/data/xyyyz-tests" >> ~/.bash_profile
												  # source ~/.bash_profile
												  # Remove option t if you're running it on jenkins
												  # e.g. docker run -v /tmp:/tmp -i <IMAGE_ID> /bin/bash
												  # -t, –tty Allocate a pseudo-TTY
												  # So you will get "the input device is not a TTY" error message.
nosetests -sv end_to_end_tests/substrate_plus_layers/acceptance/water_and_island/test_water_and_island_feature_sampling_real_data.py:TestWaterIslandFeatureSampling.test_200_create_project

OR

docker run -d <image_id>								# Run Docker image
docker ps										# Check the container(running image)
docker info										# Check number of containers running, paused and stopped.Total no of images
docker exec -it f151f4ab7085 /bin/bash							# Where f151f4ab7085 is container id; Invoke the shell of container	

docker container stop <CONTAINER_ID>							# Stop a container
docker container rm $(docker container ps -a | grep Exited | awk '{print $1}')		# Delete all stopped/exited containers
docker image rm $(docker images | grep -i flask-microservice | awk '{print $3}') -f     # Forcibly remove specific docker images

docker system prune									# remove all the stopped containers, all the networks that are not used, 											 # all dangling images and all build caches


HOW TO ATTACH TO AN EXISTING CONTAINER:
=======================================
$ docker container ls -a
CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                      PORTS                    NAMES
0ebce47a9f3a        postgres            "docker-entrypoint.s…"   About a minute ago   Up 55 seconds               0.0.0.0:5432->5432/tcp   online-exam-db
1209670d1088        ubuntu              "/bin/bash"              3 days ago           Exited (255) 24 hours ago                            hopeful_vaughan

$ docker container start 0ebce47a9f3a
0ebce47a9f3a
$ docker exec -it 0ebce47a9f3a /bin/bash

DOCKER COMPOSE:
===============
docker-compose up			docker-compose up -d			=> detached mode
docker-compose stop
docker-compose down

docker volume prune								# remove all local volumes not used by at least one container


docker-compose -f stg/docker-compose.stg.yml up					# file in a separate directory or a different file name
docker-compose -f docker-compose.yml -f prod/docker-compose.prod.yml up		# values defined in the second compose file override the ones in the first


COMMONLY USED COMMANDS WITH OUTPUT:
==================================
$ docker login docker.apple.com
$ docker pull docker.apple.com/neutronbot/reactor-x:version-3.2
$ docker images
REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE
docker.apple.com/neutronbot/reactor-x   version-3.2         535022205989        21 minutes ago      1.87GB
friendlyhello                           latest              51569f6901bb        6 months ago        132MB
samirsahoo/get-started                  part2               51569f6901bb        6 months ago        132MB
python                                  2.7-slim            c9cde4658340        6 months ago        120MB
workspace                               latest              262b7ab69ffb        8 months ago        989MB
python                                  latest              17453243214e        8 months ago        916MB
python                                  2.7                 0fcc7acd124b        8 months ago        902MB
hello-world                             latest              e38bc07ac18e        11 months ago       1.85kB

$ docker run -d 535022205989
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                          NAMES
f151f4ab7085        535022205989        "sh start-all.sh"   10 seconds ago      Up 9 seconds        22/tcp, 18001/tcp, 18080/tcp   angry_chandrasekhar

$ docker exec -it f151f4ab7085 /bin/bash
[root@f151f4ab7085 /]# python
Python 2.7.14 (default, Jun  8 2018, 22:19:22) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import pandas
>>> 
=======================================

## How to build?##
Build images: docker build .
###
docker build -t friendlyhello .  # Create image using this directory's Dockerfile
docker run -p 4000:80 friendlyhello  # Run "friendlyname" mapping port 4000 to 80
docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode
docker container ls                                # List all running containers
docker container ls -a             # List all containers, even those not running
docker container stop <hash>           # Gracefully stop the specified container
docker container kill <hash>         # Force shutdown of the specified container
docker container rm <hash>        # Remove specified container from this machine
docker container rm $(docker container ls -a -q)         # Remove all containers
docker images					# List active images. This is same as "docker image ls"
docker image ls -a                             # List all images on this machine
docker image rm <image id>            # Remove specified image from this machine
docker image rm $(docker image ls -a -q)   # Remove all images from this machine
docker login             # Log in this CLI session using your Docker credentials
docker tag <image> username/repository:tag  # Tag <image> for upload to registry
docker push username/repository:tag            # Upload tagged image to registry
docker run username/repository:tag 

## Display Docker version and info
docker --version
docker version
docker info

## This works only if you have a local image. Give your environment a quick test run to make sure you’re all set up:(Execute Docker image)
docker run hello-world

## List Docker containers (running, all, all in quiet mode)
docker container ls
docker container ls --all
docker container ls -aq

================================================================================ More details about Docker and Docker Swarm below==================================================================================================

Set up your Docker environment (on this page)
Build an image and run it as one container
Scale your app to run multiple containers
Distribute your app across a cluster
Stack services by adding a backend database
Deploy your app to production
Docker concepts
Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. The use of Linux containers to deploy applications is called containerization. Containers are not new, but their use for easily deploying applications is.

Containerization is increasingly popular because containers are:

Flexible: Even the most complex applications can be containerized.
Lightweight: Containers leverage and share the host kernel.
Interchangeable: You can deploy updates and upgrades on-the-fly.
Portable: You can build locally, deploy to the cloud, and run anywhere.
Scalable: You can increase and automatically distribute container replicas.
Stackable: You can stack services vertically and on-the-fly.
Containers are portable

Containerization makes CI/CD seamless. For example:

applications have no system dependencies
updates can be pushed to any part of a distributed application
resource density can be optimized.
With Docker, scaling your application is a matter of spinning up new executables, not running heavy VM hosts.

Images and containers
A container is launched by running an image. An image is an executable package that includes everything needed to run an application--the code, a runtime, libraries, environment variables, and configuration files.

A container is a runtime instance of an image--what the image becomes in memory when executed (that is, an image with state, or a user process). You can see a list of your running containers with the command, docker ps, just as you would in Linux.

Containers and virtual machines
A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight.

By contrast, a virtual machine (VM) runs a full-blown “guest” operating system with virtual access to host resources through a hypervisor. In general, VMs provide an environment with more resources than most applications need.

Samirs-MacBook-Pro:spark samirsahoo$ history | grep -i 'brew install'
  brew install docker
  brew install docker-machine
  brew install docker-compose

Cheat Sheet
## List Docker CLI commands
docker
docker container --help

## Display Docker version and info
docker --version
docker version
docker info

## Give your environment a quick test run to make sure you’re all set up:(Execute Docker image)
docker run hello-world

## List Docker images
docker image ls

## List Docker containers (running, all, all in quiet mode)
docker container ls
docker container ls --all
docker container ls -aq

BUILDING APP
============
It’s time to begin building an app the Docker way. We start at the bottom of the hierarchy of such an app, which is a container, which we cover on this page. Above this level is a service, which defines how containers behave in production, covered in Part 3. Finally, at the top level is the stack, defining the interactions of all the services, covered in Part 5.

In the past, if you were to start writing a Python app, your first order of business was to install a Python runtime onto your machine. But, that creates a situation where the environment on your machine needs to be perfect for your app to run as expected, and also needs to match your production environment.
With Docker, you can just grab a portable Python runtime as an image, no installation necessary. Then, your build can include the base Python image right alongside your app code, ensuring that your app, its dependencies, and the runtime, all travel together.

These portable images are defined by something called a Dockerfile.

Define a container with Dockerfile
Dockerfile defines what goes on in the environment inside your container. Access to resources like networking interfaces and disk drives is virtualized inside this environment, which is isolated from the rest of your system, so you need to map ports to the outside world, and be specific about what files you want to “copy in” to that environment. However, after doing that, you can expect that the build of your app defined in this Dockerfile behaves exactly the same wherever it runs.

Dockerfile
Create an empty directory. Change directories (cd) into the new directory, create a file called Dockerfile, copy-and-paste the following content into that file, and save it. Take note of the comments that explain each statement in your new Dockerfile.

# Use an official Python runtime as a parent image
FROM python:2.7-slim

# Set the working directory to /app. If the directory is not present create a directory first. sudo mkdir /app
WORKDIR /app

# Copy the current directory contents into the container at /app
ADD . /app

# Install any needed packages specified in requirements.txt
RUN pip install --trusted-host pypi.python.org -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# Define environment variable
ENV NAME World

# Run app.py when the container launches
CMD ["python", "app.py"]
This Dockerfile refers to a couple of files we haven’t created yet, namely app.py and requirements.txt. Let’s create those next.

The app itself
Create two more files, requirements.txt and app.py, and put them in the same folder with the Dockerfile. This completes our app, which as you can see is quite simple. When the above Dockerfile is built into an image, app.py and requirements.txt is present because of that Dockerfile’s ADD command, and the output from app.py is accessible over HTTP thanks to the EXPOSE command.

requirements.txt
Flask
Redis
app.py
from flask import Flask
from redis import Redis, RedisError
import os
import socket

# Connect to Redis
redis = Redis(host="redis", db=0, socket_connect_timeout=2, socket_timeout=2)

app = Flask(__name__)

@app.route("/")
def hello():
    try:
        visits = redis.incr("counter")
    except RedisError:
        visits = "<i>cannot connect to Redis, counter disabled</i>"

    html = "<h3>Hello {name}!</h3>" \
           "<b>Hostname:</b> {hostname}<br/>" \
           "<b>Visits:</b> {visits}"
    return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname(), visits=visits)

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80)
Now we see that pip install -r requirements.txt installs the Flask and Redis libraries for Python, and the app prints the environment variable NAME, as well as the output of a call to socket.gethostname(). Finally, because Redis isn’t running (as we’ve only installed the Python library, and not Redis itself), we should expect that the attempt to use it here fails and produces the error message.

Note: Accessing the name of the host when inside a container retrieves the container ID, which is like the process ID for a running executable.
That’s it! You don’t need Python or anything in requirements.txt on your system, nor does building or running this image install them on your system. It doesn’t seem like you’ve really set up an environment with Python and Flask, but you have.

Build the app
We are ready to build the app. Make sure you are still at the top level of your new directory. Here’s what ls should show:

$ ls
Dockerfile		app.py			requirements.txt
Now run the build command. This creates a Docker image, which we’re going to tag using -t so it has a friendly name.

docker build -t friendlyhello .
Where is your built image? It’s in your machine’s local Docker image registry:

$ docker image ls

REPOSITORY            TAG                 IMAGE ID
friendlyhello         latest              326387cea398

Proxy servers can block connections to your web app once it’s up and running. If you are behind a proxy server, add the following lines to your Dockerfile, using the ENV command to specify the host and port for your proxy servers:

# Set proxy server, replace host:port with values for your servers
ENV http_proxy host:port
ENV https_proxy host:port
DNS settings

DNS misconfigurations can generate problems with pip. You need to set your own DNS server address to make pip work properly. You might want to change the DNS settings of the Docker daemon. You can edit (or create) the configuration file at /etc/docker/daemon.json with the dns key, as following:

{
  "dns": ["your_dns_address", "8.8.8.8"]
}
In the example above, the first element of the list is the address of your DNS server. The second item is the Google’s DNS which can be used when the first one is not available.

Before proceeding, save daemon.json and restart the docker service.

sudo service docker restart

Once fixed, retry to run the build command.
Run the app
Run the app, mapping your machine’s port 4000 to the container’s published port 80 using -p:

docker run -p 4000:80 friendlyhello
You should see a message that Python is serving your app at http://0.0.0.0:80. But that message is coming from inside the container, which doesn’t know you mapped port 80 of that container to 4000, making the correct URL http://localhost:4000.

Go to that URL in a web browser to see the display content served up on a web page.

Hello World in browser

Note: If you are using Docker Toolbox on Windows 7, use the Docker Machine IP instead of localhost. For example, http://192.168.99.100:4000/. To find the IP address, use the command docker-machine ip.
You can also use the curl command in a shell to view the same content.

$ curl http://localhost:4000

<h3>Hello World!</h3><b>Hostname:</b> 8fc990912a14<br/><b>Visits:</b> <i>cannot connect to Redis, counter disabled</i>
This port remapping of 4000:80 demonstrates the difference between EXPOSE within the Dockerfile and what the publish value is set to when running docker run -p. In later steps, map port 4000 on the host to port 80 in the container and use http://localhost.

Hit CTRL+C in your terminal to quit.

Now let’s run the app in the background, in detached mode:

docker run -d -p 4000:80 friendlyhello
You get the long container ID for your app and then are kicked back to your terminal. Your container is running in the background. You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands):

$ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED
1fa4ab2cf395        friendlyhello       "python app.py"     28 seconds ago
Notice that CONTAINER ID matches what’s on http://localhost:4000.

Now use docker container stop to end the process, using the CONTAINER ID, like so:
docker container stop 1fa4ab2cf395

Share your image
To demonstrate the portability of what we just created, let’s upload our built image and run it somewhere else. After all, you need to know how to push to registries when you want to deploy containers to production.

A registry is a collection of repositories, and a repository is a collection of images—sort of like a GitHub repository, except the code is already built. An account on a registry can create many repositories. The docker CLI uses Docker’s public registry by default.

Note: We use Docker’s public registry here just because it’s free and pre-configured, but there are many public ones to choose from, and you can even set up your own private registry using Docker Trusted Registry.
Log in with your Docker ID
If you don’t have a Docker account, sign up for one at hub.docker.com. Make note of your username.

Log in to the Docker public registry on your local machine.

$ docker login

Tag the image
The notation for associating a local image with a repository on a registry is username/repository:tag. The tag is optional, but recommended, since it is the mechanism that registries use to give Docker images a version. Give the repository and tag meaningful names for the context, such as get-started:part2. This puts the image in the get-started repository and tag it as part2.

Now, put it all together to tag the image. Run docker tag image with your username, repository, and tag names so that the image uploads to your desired destination. The syntax of the command is:

docker tag image username/repository:tag
For example:

docker tag friendlyhello gordon/get-started:part2
Run docker image ls to see your newly tagged image.

$ docker image ls

REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
friendlyhello            latest              d9e555c53008        3 minutes ago       195MB
gordon/get-started         part2               d9e555c53008        3 minutes ago       195MB
python                   2.7-slim            1c7128a655f6        5 days ago          183MB
...
Publish the image
Upload your tagged image to the repository:

docker push username/repository:tag
Once complete, the results of this upload are publicly available. If you log in to Docker Hub, you see the new image there, with its pull command.

Pull and run the image from the remote repository
From now on, you can use docker run and run your app on any machine with this command:

docker run -p 4000:80 username/repository:tag
If the image isn’t available locally on the machine, Docker pulls it from the repository.

$ docker run -p 4000:80 gordon/get-started:part2
Unable to find image 'gordon/get-started:part2' locally
part2: Pulling from gordon/get-started
10a267c67f42: Already exists
f68a39a6a5e4: Already exists
9beaffc0cf19: Already exists
3c1fe835fb6b: Already exists
4c9f1fa8fcb8: Already exists
ee7d8f576a14: Already exists
fbccdcced46e: Already exists
Digest: sha256:0601c866aab2adcc6498200efd0f754037e909e5fd42069adeff72d1e2439068
Status: Downloaded newer image for gordon/get-started:part2
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
No matter where docker run executes, it pulls your image, along with Python and all the dependencies from requirements.txt, and runs your code. It all travels together in a neat little package, and you don’t need to install anything on the host machine for Docker to run it.

Recap and cheat sheet (optional)
Here is a list of the basic Docker commands from this page, and some related ones if you’d like to explore a bit before moving on.

docker build -t friendlyhello .  # Create image using this directory's Dockerfile
docker run -p 4000:80 friendlyhello  # Run "friendlyname" mapping port 4000 to 80
docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode
docker container ls                                # List all running containers
docker container ls -a             # List all containers, even those not running
docker container stop <hash>           # Gracefully stop the specified container
docker container kill <hash>         # Force shutdown of the specified container
docker container rm <hash>        # Remove specified container from this machine
docker container rm $(docker container ls -a -q)         # Remove all containers
docker image ls -a                             # List all images on this machine
docker image rm <image id>            # Remove specified image from this machine
docker image rm $(docker image ls -a -q)   # Remove all images from this machine
docker login             # Log in this CLI session using your Docker credentials
docker tag <image> username/repository:tag  # Tag <image> for upload to registry
docker push username/repository:tag            # Upload tagged image to registry
docker run username/repository:tag                   # Run image from a registry

======================================================================================= SCALING APPLICATION USING DOCKER ======================================

In part 3, we scale our application and enable load-balancing. To do this, we must go one level up in the hierarchy of a distributed application: the service.

About services
In a distributed application, different pieces of the app are called “services.” For example, if you imagine a video sharing site, it probably includes a service for storing application data in a database, a service for video transcoding in the background after a user uploads something, a service for the front-end, and so on.

Services are really just “containers in production.” A service only runs one image, but it codifies the way that image runs—what ports it should use, how many replicas of the container should run so the service has the capacity it needs, and so on. Scaling a service changes the number of container instances running that piece of software, assigning more computing resources to the service in the process.

Luckily it’s very easy to define, run, and scale services with the Docker platform -- just write a docker-compose.yml file.

Your first docker-compose.yml file
A docker-compose.yml file is a YAML file that defines how Docker containers should behave in production.

docker-compose.yml
Save this file as docker-compose.yml wherever you want. Be sure you have pushed the image you created in Part 2 to a registry, and update this .yml by replacing username/repo:tag with your image details.

version: "3"
services:
  web:
    # replace username/repo:tag with your name and image details
    image: username/repo:tag
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: "0.1"
          memory: 50M
      restart_policy:
        condition: on-failure
    ports:
      - "4000:80"
    networks:
      - webnet
networks:
  webnet:

This docker-compose.yml file tells Docker to do the following:

Pull the image we uploaded in step 2 from the registry.

Run 5 instances of that image as a service called web, limiting each one to use, at most, 10% of the CPU (across all cores), and 50MB of RAM.

Immediately restart containers if one fails.

Map port 4000 on the host to web’s port 80.

Instruct web’s containers to share port 80 via a load-balanced network called webnet. (Internally, the containers themselves publish to web’s port 80 at an ephemeral port.)

Define the webnet network with the default settings (which is a load-balanced overlay network).

Run your new load-balanced app
Before we can use the docker stack deploy command we first run:

docker swarm init
Note: We get into the meaning of that command in part 4. If you don’t run docker swarm init you get an error that “this node is not a swarm manager.”
Now let’s run it. You need to give your app a name. Here, it is set to getstartedlab:

docker stack deploy -c docker-compose.yml getstartedlab
Our single service stack is running 5 container instances of our deployed image on one host. Let’s investigate.

Get the service ID for the one service in our application:

docker service ls
Look for output for the web service, prepended with your app name. If you named it the same as shown in this example, the name is getstartedlab_web. The service ID is listed as well, along with the number of replicas, image name, and exposed ports.

A single container running in a service is called a task. Tasks are given unique IDs that numerically increment, up to the number of replicas you defined in docker-compose.yml. List the tasks for your service:

docker service ps getstartedlab_web
Tasks also show up if you just list all the containers on your system, though that is not filtered by service:

docker container ls -q
You can run curl -4 http://localhost:4000 several times in a row, or go to that URL in your browser and hit refresh a few times.

Hello World in browser

Either way, the container ID changes, demonstrating the load-balancing; with each request, one of the 5 tasks is chosen, in a round-robin fashion, to respond. The container IDs match your output from the previous command (docker container ls -q).

Scale the app
You can scale the app by changing the replicas value in docker-compose.yml, saving the change, and re-running the docker stack deploy command:

docker stack deploy -c docker-compose.yml getstartedlab

Docker performs an in-place update, no need to tear the stack down first or kill any containers.

Now, re-run docker container ls -q to see the deployed instances reconfigured. If you scaled up the replicas, more tasks, and hence, more containers, are started.

Take down the app and the swarm
Take the app down with docker stack rm:

docker stack rm getstartedlab

Take down the swarm.

docker swarm leave --force

It’s as easy as that to stand up and scale your app with Docker. You’ve taken a huge step towards learning how to run containers in production. Up next, you learn how to run this app as a bonafide swarm on a cluster of Docker machines.

Note: Compose files like this are used to define applications with Docker, and can be uploaded to cloud providers using Docker Cloud, or on any hardware or cloud provider you choose with Docker Enterprise Edition.

Recap and cheat sheet (optional)
Here’s a terminal recording of what was covered on this page:

docker stack ls                                            # List stacks or apps
docker stack deploy -c <composefile> <appname>  # Run the specified Compose file
docker service ls                 # List running services associated with an app
docker service ps <service>                  # List tasks associated with an app
docker inspect <task or container>                   # Inspect task or container
docker container ls -q                                      # List container IDs
docker stack rm <appname>                             # Tear down an application
docker swarm leave --force      # Take down a single node swarm from the manager

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In part 3, you took an app you wrote in part 2, and defined how it should run in production by turning it into a service, scaling it up 5x in the process.
Here in part 4, you deploy this application onto a cluster, running it on multiple machines. Multi-container, multi-machine applications are made possible by joining multiple machines into a “Dockerized” cluster called a swarm.

Understanding Swarm clusters
A swarm is a group of machines that are running Docker and joined into a cluster. After that has happened, you continue to run the Docker commands you’re used to, but now they are executed on a cluster by a swarm manager. The machines in a swarm can be physical or virtual. After joining a swarm, they are referred to as nodes.

Swarm managers can use several strategies to run containers, such as “emptiest node” -- which fills the least utilized machines with containers. Or “global”, which ensures that each machine gets exactly one instance of the specified container. You instruct the swarm manager to use these strategies in the Compose file, just like the one you have already been using.

Swarm managers are the only machines in a swarm that can execute your commands, or authorize other machines to join the swarm as workers. Workers are just there to provide capacity and do not have the authority to tell any other machine what it can and cannot do.

Up until now, you have been using Docker in a single-host mode on your local machine. But Docker also can be switched into swarm mode, and that’s what enables the use of swarms. Enabling swarm mode instantly makes the current machine a swarm manager. From then on, Docker runs the commands you execute on the swarm you’re managing, rather than just on the current machine.

Set up your swarm
A swarm is made up of multiple nodes, which can be either physical or virtual machines. The basic concept is simple enough: run docker swarm init to enable swarm mode and make your current machine a swarm manager, then run docker swarm join on other machines to have them join the swarm as workers. Choose a tab below to see how this plays out in various contexts. We use VMs to quickly create a two-machine cluster and turn it into a swarm.

Create a cluster
Local VMs (Mac, Linux, Windows 7 and 8)
Local VMs (Windows 10/Hyper-V)

You need a hypervisor that can create virtual machines (VMs), so install Oracle VirtualBox for your machine’s OS.

Note: If you are on a Windows system that has Hyper-V installed, such as Windows 10, there is no need to install VirtualBox and you should use Hyper-V instead. View the instructions for Hyper-V systems by clicking the Hyper-V tab above. If you are using Docker Toolbox, you should already have VirtualBox installed as part of it, so you are good to go.
Now, create a couple of VMs using docker-machine, using the VirtualBox driver:

Make sure virtualbox is installed. Install it by "brew install virtualbox"
$ docker-machine create --driver virtualbox myvm1
$ docker-machine create --driver virtualbox myvm2

$ docker-machine ls
NAME    ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS
myvm1   -        virtualbox   Running   tcp://192.168.99.100:2376           v17.06.2-ce
myvm2   -        virtualbox   Running   tcp://192.168.99.101:2376           v17.06.2-ce

$ docker-machine ssh myvm1											# You can ssh to your vm
$ docker swarm init --advertise-addr 192.168.99.100								# INITIALIZE THE SWARM
Swarm initialized: current node (qb4bx0u1y2hvw9k0s833vtdll) is now a manager.					# It can be directly done by #docker-machine ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>"
To add a worker to this swarm, run the following command:							# Swarm initialized: current node <node ID> is now a manager.

docker swarm join --token SWMTKN-1-682md5kvvb5vk2h0dslux3v5bx8dnfhlbn2yomxx8gkujxhlhp-2npfa46roxy9okimbhsi239j2 192.168.99.100:2377			

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.		# The first machine acts as the manager, which executes management commands 
														# and authenticates workers to join the swarm, and the second is a worker.

														# docker-machine ssh myvm1 "pwd" => Sending command to docker machine
$ CTRL+D													# logout of myvm1
$ docker-machine ssh myvm2											# login to myvm2 to add it as a worker
$ docker swarm join --token SWMTKN-1-682md5kvvb5vk2h0dslux3v5bx8dnfhlbn2yomxx8gkujxhlhp-2npfa46roxy9okimbhsi239j2 192.168.99.100:2377

														# This node joined a swarm as a worker.

Congratulations, you have created your first swarm!

$ docker-machine ssh myvm1 "docker node ls"									# View the nodes in this swarm
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
qb4bx0u1y2hvw9k0s833vtdll *   myvm1               Ready               Active              Leader              18.09.4
xe4u1yp2egbbvw87fadnmm2cy     myvm2               Ready               Active                                  18.09.4

Notes:
Deploy your app on the swarm cluster.
If you want to start over, you can run "docker swarm leave" from each node to leave swarm
Just remember that only swarm managers like myvm1 execute Docker commands; workers are just for capacity.
Always run docker swarm init and docker swarm join with port 2377 (the swarm management port), or no port at all and let it take the default.
The machine IP addresses returned by docker-machine ls include port 2376, which is the Docker daemon port. Do not use this port or you may experience errors.

$ docker-machine --native-ssh ssh myvm1										# Having trouble using SSH? Try the --native-ssh flag. Option lets you use your own system’s SSH

$ docker-machine env myvm1											# Run docker-machine env myvm1 to get the command to configure your shell to talk to myvm1.
export DOCKER_TLS_VERIFY="1"
export DOCKER_HOST="tcp://192.168.99.100:2376"
export DOCKER_CERT_PATH="/Users/sam/.docker/machine/machines/myvm1"
export DOCKER_MACHINE_NAME="myvm1"

# Run the below command to configure your shell to talk to myvm1.
$ eval $(docker-machine env myvm1)
$ docker-machine ls												# Run docker-machine ls to verify that myvm1 is now the active machine, as indicated by the asterisk next to it.
NAME    ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS
myvm1   *        virtualbox   Running   tcp://192.168.99.100:2376           v17.06.2-ce
myvm2   -        virtualbox   Running   tcp://192.168.99.101:2376           v17.06.2-ce

Deploy the app on the swarm manager
Now that you have myvm1, you can use its powers as a swarm manager to deploy your app by using the same docker stack deploy command you used in part 3 to myvm1, and your local copy of docker-compose.yml.. This command may take a few seconds to complete and the deployment takes some time to be available. Use the docker service ps <service_name> command on a swarm manager to verify that all services have been redeployed.
You are connected to myvm1 by means of the docker-machine shell configuration, and you still have access to the files on your local host. Make sure you are in the same directory as before, which includes the docker-compose.yml file you created in part 3.
$ pwd
/Users/samirsahoo
$ ls docker-compose.yml 
docker-compose.yml
-------------------- docker-compose.yml ------------
version: "3"
services:
  web:
    # replace username/repo:tag with your name and image details
    image: username/repo:tag
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: "0.1"
          memory: 50M
      restart_policy:
        condition: on-failure
    ports:
      - "4000:80"
    networks:
      - webnet
networks:
  webnet:
---------------------------------------------------

$ docker stack deploy -c docker-compose.yml getstartedlab							# Just like before, run the following command to deploy the app on myvm1.
							
And that’s it, the app is deployed on a swarm cluster!
Note: If your image is stored on a private registry instead of Docker Hub, you need to be logged in using docker login <your-registry> and then you need to add the --with-registry-auth flag to the above command. For example:

$docker login registry.example.com
$ docker stack deploy --with-registry-auth -c docker-compose.yml getstartedlab

This passes the login token from your local client to the swarm nodes where the service is deployed, using the encrypted WAL logs. With this information, the nodes are able to log into the registry and pull the image.
Now you can use the same docker commands you used in part 3. Only this time notice that the services (and associated containers) have been distributed between both myvm1 and myvm2.

$ docker stack ps getstartedlab
ID                  NAME                  IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS
41gg80dlbm0i        getstartedlab_web.1   username/repo:tag   myvm2               Running             Preparing 3 minutes ago                       
t6o0o4swtdpk        getstartedlab_web.2   username/repo:tag   myvm1               Running             Preparing 3 minutes ago                       
kf7zfclcgym4        getstartedlab_web.3   username/repo:tag   myvm2               Running             Preparing 3 minutes ago                       
p066do93mjme        getstartedlab_web.4   username/repo:tag   myvm2               Running             Preparing 3 minutes ago                       
nk3znf5ugone        getstartedlab_web.5   username/repo:tag   myvm1               Running             Preparing 3 minutes ago
  
Connecting to VMs with docker-machine env and docker-machine ssh

To set your shell to talk to a different machine like myvm2, simply re-run docker-machine env in the same or a different shell, then run the given command to point to myvm2. This is always specific to the current shell. If you change to an unconfigured shell or open a new one, you need to re-run the commands. Use docker-machine ls to list machines, see what state they are in, get IP addresses, and find out which one, if any, you are connected to. To learn more, see the Docker Machine getting started topics.
Alternatively, you can wrap Docker commands in the form of docker-machine ssh <machine> "<command>", which logs directly into the VM but doesn’t give you immediate access to files on your local host.
On Mac and Linux, you can use docker-machine scp <file> <machine>:~ to copy files across machines, but Windows users need a Linux terminal emulator like Git Bash for this to work.

Accessing your cluster
You can access your app from the IP address of either myvm1 or myvm2.

The network you created is shared between them and load-balancing. Run docker-machine ls to get your VMs’ IP addresses and visit either of them on a browser, hitting refresh (or just curl them).

Hello World in browser

There are five possible container IDs all cycling by randomly, demonstrating the load-balancing.

The reason both IP addresses work is that nodes in a swarm participate in an ingress routing mesh. This ensures that a service deployed at a certain port within your swarm always has that port reserved to itself, no matter what node is actually running the container. Here’s a diagram of how a routing mesh for a service called my-web published at port 8080 on a three-node swarm would look:

routing mesh diagram

Having connectivity trouble?

Keep in mind that to use the ingress network in the swarm, you need to have the following ports open between the swarm nodes before you enable swarm mode:

Port 7946 TCP/UDP for container network discovery.
Port 4789 UDP for the container ingress network.
Iterating and scaling your app
From here you can do everything you learned about in parts 2 and 3.

Scale the app by changing the docker-compose.yml file.

Change the app behavior by editing code, then rebuild, and push the new image. (To do this, follow the same steps you took earlier to build the app and publish the image).

In either case, simply run docker stack deploy again to deploy these changes.

You can join any machine, physical or virtual, to this swarm, using the same docker swarm join command you used on myvm2, and capacity is added to your cluster. Just run docker stack deploy afterwards, and your app can take advantage of the new resources.

Cleanup and reboot
Stacks and swarms
You can tear down the stack with docker stack rm. For example:

docker stack rm getstartedlab
Keep the swarm or remove it?

At some point later, you can remove this swarm if you want to with docker-machine ssh myvm2 "docker swarm leave" on the worker and docker-machine ssh myvm1 "docker swarm leave --force" on the manager, but you need this swarm for part 5, so keep it around for now.
Unsetting docker-machine shell variable settings
You can unset the docker-machine environment variables in your current shell with the given command.

On Mac or Linux the command is:

  eval $(docker-machine env -u)
On Windows the command is:

  & "C:\Program Files\Docker\Docker\Resources\bin\docker-machine.exe" env -u | Invoke-Expression
This disconnects the shell from docker-machine created virtual machines, and allows you to continue working in the same shell, now using native docker commands (for example, on Docker for Mac or Docker for Windows). To learn more, see the Machine topic on unsetting environment variables.

Restarting Docker machines
If you shut down your local host, Docker machines stops running. You can check the status of machines by running docker-machine ls.

$ docker-machine ls
NAME    ACTIVE   DRIVER       STATE     URL   SWARM   DOCKER    ERRORS
myvm1   -        virtualbox   Stopped                 Unknown
myvm2   -        virtualbox   Stopped                 Unknown
To restart a machine that’s stopped, run:

docker-machine start <machine-name>
For example:

$ docker-machine start myvm1
Starting "myvm1"...
(myvm1) Check network to re-create if needed...
(myvm1) Waiting for an IP...
Machine "myvm1" was started.
Waiting for SSH to be available...
Detecting the provisioner...
Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.

$ docker-machine start myvm2
Starting "myvm2"...
(myvm2) Check network to re-create if needed...
(myvm2) Waiting for an IP...
Machine "myvm2" was started.
Waiting for SSH to be available...
Detecting the provisioner...
Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.
On to Part 5 >>

Recap and cheat sheet (optional)
In part 4 you learned what a swarm is, how nodes in swarms can be managers or workers, created a swarm, and deployed an application on it. You saw that the core Docker commands didn’t change from part 3, they just had to be targeted to run on a swarm master. You also saw the power of Docker’s networking in action, which kept load-balancing requests across containers, even though they were running on different machines. Finally, you learned how to iterate and scale your app on a cluster.

Here are some commands you might like to run to interact with your swarm and your VMs a bit:

docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux)
docker-machine create -d hyperv --hyperv-virtual-switch "myswitch" myvm1 # Win10
docker-machine env myvm1                # View basic information about your node
docker-machine ssh myvm1 "docker node ls"         # List the nodes in your swarm
docker-machine ssh myvm1 "docker node inspect <node ID>"        # Inspect a node
docker-machine ssh myvm1 "docker swarm join-token -q worker"   # View join token
docker-machine ssh myvm1   # Open an SSH session with the VM; type "exit" to end
docker node ls                # View nodes in swarm (while logged on to manager)
docker-machine ssh myvm2 "docker swarm leave"  # Make the worker leave the swarm
docker-machine ssh myvm1 "docker swarm leave -f" # Make master leave, kill swarm
docker-machine ls # list VMs, asterisk shows which VM this shell is talking to
docker-machine start myvm1            # Start a VM that is currently not running
docker-machine env myvm1      # show environment variables and command for myvm1
eval $(docker-machine env myvm1)         # Mac command to connect shell to myvm1
& "C:\Program Files\Docker\Docker\Resources\bin\docker-machine.exe" env myvm1 | Invoke-Expression   # Windows command to connect shell to myvm1
docker stack deploy -c <file> <app>  # Deploy an app; command shell must be set to talk to manager (myvm1), uses local Compose file
docker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir (only required if you use ssh to connect to manager and deploy the app)
docker-machine ssh myvm1 "docker stack deploy -c <file> <app>"   # Deploy an app using ssh (you must have first copied the Compose file to myvm1)
eval $(docker-machine env -u)     # Disconnect shell from VMs, use native docker
docker-machine stop $(docker-machine ls -q)               # Stop all running VMs
docker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images
============= Continue from here ===============

$ brew search nginx
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
$ docker run -d -p 80:80 --name webserver nginx
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
8882d415836d        nginx               "nginx -g 'daemon of…"   42 seconds ago      Up 41 seconds       0.0.0.0:80->80/tcp   webserver

Now you can open the nginx server running. Open webpage http://localhost

Stop or remove containers and images.

The nginx webserver will continue to run in the container on that port until you stop and/or remove the container. If you want to stop the webserver, type: docker stop webserver and start it again with docker start webserver. A stopped container will not show up with docker ps; for that, you need to run docker ps -a.

To stop and remove the running container with a single command, type: docker rm -f webserver. This will remove the container, but not the nginx image. You can list local images with docker images. You might want to keep some images around so that you don’t have to pull them again from Docker Hub. To remove an image you no longer need, use docker rmi followed by an image ID or image name. For example, docker rmi nginx.

========================== Note ============================


When I restarted Docker Machine and the Docker image for MySQL, none of my changes were present in the mapped directory! The only databases and tables where those created through the Dockerfile. All other changes had been lost! The only reason I was using volumes was to not have this problem and yet here the problem was!

I inspected the container and didn’t see any issues with the mounts. My friend Michael did some more digging around and found that the file system mounted on root was tmpfs. Since files stored in tmpfs are stored in volatile memory, they are wiped on a restart. Running a df on the host revealed the issue more clearly:

df -h
Filesystem                Size      Used Available Use% Mounted on
tmpfs                     1.8G    115.3M      1.6G   6% /
tmpfs                  1001.4M     64.0K   1001.3M   0% /dev/shm
/dev/sda1                18.2G      2.2G     15.0G  13% /mnt/sda1
cgroup                 1001.4M         0   1001.4M   0% /sys/fs/cgroup
none                    280.3G    113.7G    166.6G  41% /Users
/dev/sda1                18.2G      2.2G     15.0G  13% /mnt/sda1/var/lib/docker/aufs
The only safe place to store files seems to be in the /mnt directory. I had two options going forward:

Map the MySQL data directory as a volume to a subdirectory under /mnt directory on the host.
Expose the MySQL data directory as a volume without an explicit host map.
I chose to go with option 2 as on my laptop as it involved less stuffing around and I knew it would be persisted between restarts.

