df - A pandas DataFrame object s - A pandas Series object (https://s3.amazonaws.com/dq-blog-files/pandas-cheat-sheet.pdf)

IMPORTS
=======
import pandas as pd
import numpy as np

IMPORTING DATA
==============
pd.read_csv(filename) 			- From a CSV file 
pd.read_table(filename) 		- From a delimited text file (like TSV)
pd.read_excel(filename) 		- From an Excel file
pd.read_sql(query, connection_object) 	- Reads from a SQL table/database 
pd.read_json(json_string) 		- Reads from a JSON formatted string, URL or file. 
pd.read_html(url) 			- Parses an html URL, string or file and extracts tables to a list of dataframes 
pd.read_clipboard() 			- Takes the contents of your clipboard and passes it to read_table() 
pd.DataFrame(dict) 			- From a dict, keys for columns names, values for data as lists

EXPORTING DATA
==============
df.to_csv(filename) 				- Writes to a CSV file 
df.to_excel(filename) 				- Writes to an Excel file 
df.to_sql(table_name, connection_object) 	- Writes to a SQL table 
df.to_json(filename) 				- Writes to a file in JSON format
df.to_html(filename) 				- Saves as an HTML table 
df.to_clipboard() 				- Writes to the clipboard

Read and Write to SQL Query or Database Table
=============================================
read_sql()is a convenience wrapper around read_sql_table() and read_sql_query()
 >>> pd.read_csv(' le.csv', header=None, nrows=5) >>> df.to_csv('myDataFrame.csv')
>>> pd.read_excel(' le.xlsx')
>>> pd.to_excel('dir/myDataFrame.xlsx', sheet_name='Sheet1')

Read multiple sheets from the same  le
>>> xlsx = pd.ExcelFile(' le.xls')
>>> df = pd.read_excel(xlsx, 'Sheet1')
>>> from sqlalchemy import create_engine			- SQLAlchemy is an open-source SQL toolkit and object-relational mapper (ORM) for the Python 
>>> engine = create_engine('sqlite:///:memory:'			- SQLAlchemy is a library that facilitates the communication between Python programs and databases. 
								- Most of the times, this library is used as an Object Relational Mapper (ORM) tool that translates 
								- Python classes to tables on relational databases and automatically converts function calls to SQL statements.)
								- It's mostly used to define a relationship between an object/class and the database table where it's data should be saved.
								Does Django use SQLAlchemy?
								In some cases, Django and SQLAlchemy can be used together. The main use case I got to see numerous times in the real world is 
								when Django is used for all regular CRUD operations, while SQLAlchemy is used for the more complex queries, usually read-only queries

>>> pd.read_sql("SELECT * FROM my_table;", engine)
>>> pd.read_sql_table('my_table', engine)
>>> pd.read_sql_query("SELECT * FROM my_table;", engine)

CREATE TEST OBJECTS(Useful for testing)
=======================================
pd.DataFrame(np.random.rand(20,5)) 				- 5 columns and 20 rows of random floats
pd.Series(my_list) 						- Creates a series from an iterable my_list
df.index = pd.date_range('1900/1/30', periods=df.shape[0]) 	- Adds a date index

VIEWING/INSPECTING DATA
=======================
df.show()				- Displaying the data
df.head(n) 				- First n rows of the DataFrame 
df.tail(n) 				- Last n rows of the DataFrame 
df.shape() 				- Number of rows and columns 
df.info() 				- Index, Datatype and Memory information
df.describe() 				- Summary statistics for numerical columns
s.value_counts(dropna=False) 		- Views unique values and counts 
df.apply(pd.Series.value_counts) 	- Unique values and counts for all columns

SELECTION
=========
df[col] 			- Returns column with label col as Series df[[col1, col2]] - Returns Columns as a new DataFrame
s.iloc[0] 			- Selection by position 
s.loc[0] 			- Selection by index 
df.iloc[0,:] 			- First row
df.iloc[0,0] 			- First element of first column

DATA CLEANING
=============
df.columns = ['a','b','c'] 			- Renames columns pd.isnull() - Checks for null Values, Returns Boolean Array
pd.notnull() 					- Opposite of s.isnull() df.dropna() - Drops all rows that contain null values
df.dropna(axis=1)				- Drops all columns that contain null values 
df.dropna(axis=1,thresh=n) 			- Drops all rows have less than n non null values 
df.fillna(x) 					- Replaces all null values with x 
s.fillna(s.mean()) 				- Replaces all null values with the mean (mean can be replaced with almost any function from the statistics section) 
s.astype(float) 				- Converts the datatype of the series to float
s.replace(1,'one') 				- Replaces all values equal to 1 with 'one' 
s.replace([1,3],['one','three']) 		- Replaces all 1 with 'one' and 3 with 'three' 
df.rename(columns=lambda x: x + 1) 		- Mass renaming of columns
df.rename(columns={'old_name': 'new_ name'}) 	- Selective renaming
df.set_index('column_one') 			- Changes the index 
df.rename(index=lambda x: x + 1) 		- Mass renaming of index

FILTER, SORT, & GROUPBY
=======================
df[df[col] > 0.5] 						- Rows where the col column is greater than 0.5
df[(df[col] > 0.5) & (df[col] < 0.7)] 				- Rows where 0.7 > col > 0.5
df.sort_values(col1) 						- Sorts values by col1 in ascending order
df.sort_values(col2,ascending=False) 				- Sorts values by col2 in descending order
df.sort_values([col1,col2], ascending=[True,False]) 		- Sorts values by
df.groupby(col) 						- Returns a groupby object for values from one column 
df.groupby([col1,col2]) 					- Returns a groupby object values from multiple columns 
df.groupby(col1)[col2].mean() 					- Returns the mean of the values in col2, grouped by the values in col1 (mean can be replaced with almost any function from the statistics section)
df.pivot_table(index=col1,values= [col2,col3],aggfunc=mean) 	- Creates a pivot table that groups by col1 and calculates the mean of col2 and col3
df.groupby(col1).agg(np.mean) 					- Finds the average across all columns for every unique column 1 group
df.apply(np.mean) 						- Applies a function across each column
df.apply(np.max, axis=1) 					- Applies a function across each row

APPLYING FUNCTIONS
==================
>>> f = lambda x: x*2
>>> df.apply(f)
>>> df.applymap(f)

JOIN/COMBINE
============
df1.append(df2) 			- Adds the rows in df1 to the end of df2 (columns should be identical)
pd.concat([df1, df2],axis=1) 		- Adds the columns in df1 to the end of df2 (rows should be identical)
df1.join(df2,on=col1,how='inner') 	- SQL-style joins the columns in df1 with the columns on df2 where the rows for col have identical values. how can be one of 'left', 'right', 'outer', 'inner'

>>> s3 = pd.Series([7, -2, 3], index=['a', 'c', 'd'])
>>> s + s3
a 10.0 b NaN c 5.0 d 7.0

STATISTICS(These can all be applied to a series as well.)
==========
df.describe() 				- Summary statistics for numerical columns
df.mean() 				- Returns the mean of all columns 
df.corr() 				- Returns the correlation between columns in a DataFrame
df.count() 				- Returns the number of non-null values in each DataFrame column 
df.max() 				- Returns the highest value in each column
df.min() 				- Returns the lowest value in each column 
df.median() 				- Returns the median of each column 
df.std() 				- Returns the standard deviation of each column

PLOTTING (matplotlib musst be installed for plotting)
========
df.plot.hist()				- Histogram for each column
df.plot.scatter(x='w',y='h')		- Scatter chart using pairs of points

========================================================== pyspark =================================================================
Initializing Spark:
>>> from pyspark import SparkContext
>>> sc = SparkContext(master = 'local[2]')

Inspect SparkContext:
====================
>>> sc.version					Retrieve SparkContext version
>>> sc.pythonVer				Retrieve Python version
>>> sc.master					Master URL to connect to
>>> str(sc.sparkHome)				Path where Spark is installed on worker nodes Retrieve name of the Spark User running SparkContext
>>> str(sc.sparkUser())				
>>> sc.appName					Return application name
>>> sc.applicationId				Retrieve application ID
>>> sc.defaultParallelism			Return default level of parallelism
>>> sc.defaultMinPartitions			Default minimum number of partitions for RDDs

Configuration:
=============
>>> from pyspark import SparkConf, SparkContext
>>> conf = (SparkConf()
	     .setMaster("local")
	     .setAppName("My app")
	     .set("spark.executor.memory", "1g"))
>>> sc = SparkContext(conf = conf)

Using the Shell:
==============
In the PySpark shell, a special interpreter-aware SparkContext is already created in the variable called sc.
$ ./bin/spark-shell --master local[2]
$ ./bin/pyspark --master local[4] --py- les code.py
Set which master the context connects to with the --master argument, and add Python .zip, .egg or .py  les to the runtime path by passing a comma-separated list to --py- les.

Loading Data:
============
Parallelized Collections
>>> rdd = sc.parallelize([('a',7),('a',2),('b',2)])
>>> rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])
>>> rdd3 = sc.parallelize(range(100))
>>> rdd4 = sc.parallelize([("a",["x","y","z"]),("b",["p", "r"])])

External Data:
Read either one text  file from HDFS, a local  file system or or any Hadoop-supported  file system URI with textFile(), or read in a directory of text  files with wholeTextFiles().

>>> textFile = sc.textFile("/my/directory/*.txt")
>>> textFile2 = sc.wholeTextFiles("/my/directory/")

Retrieving RDD Information:
==========================
Basic Information:
>>> rdd.getNumPartitions()						List the number of partitions Count RDD instances
>>> rdd.count()								Count RDD instances by key Count RDD instances by value
 3
>>> rdd.countByKey()							Return (key,value) pairs as a dictionary
defaultdict(<type 'int'>,{'a':2,'b':1}) 

>>> rdd.countByValue()
defaultdict(<type 'int'>,{('b',2):1,('a',2):1,('a',7):1}) 

>>> rdd.collectAsMap()
 {'a': 2,'b': 2}
>>> rdd3.sum()								Sum of RDD elements
 4950
>>> sc.parallelize([]).isEmpty()					Check whether RDD is empty
True

Summary:
=======
>>> rdd3.max()			# 99 # Maximum value of RDD elements 
>>> rdd3.min()			#0 #Minimum value of RDD elements 
>>> rdd3.mean()			#49.5 #Mean value of RDD elements 
>>> rdd3.stdev()		# 28.866070047722118 #Standard deviation of RDD elements 
>>> rdd3.variance()		# 833.25 #Compute variance of RDD elements 
>>> rdd3.histogram(3)		#([0,33,66,99],[33,33,34]) #Compute histogram by bins
>>> rdd3.stats()		#Summary statistics (count, mean, stdev, max & min)

Applying Functions:
==================
rdd.map(lambda x: x+(x[1],x[0])).collect()						# Apply a function to each RDD element 
>>> conf = (SparkConf()
[('a',7,7,'a'),('a',2,2,'a'),('b',2,2,'b')] 

>>> rdd5 = rdd. atMap(lambda x: x+(x[1],x[0]))
>>> rdd5.collect()
  ['a',7,7,'a','a',2,2,'a','b',2,2,'b']

>>> rdd4.flatMapValues(lambda x: x).collect()						# Apply a function to each RDD element and  flatten the result
[('a','x'),('a','y'),('a','z'),('b','p'),('b','r')]					# Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys

Selecting Data:
==============
Getting
>>> rdd.collect()
  [('a', 7), ('a', 2), ('b', 2)]
>>> rdd.take(2)
  [('a', 7), ('a', 2)]
>>> rdd.first()
  ('a', 7)
 
>>> rdd.top(2)
  [('b', 2), ('a', 7)]

Sampling:
>>> rdd3.sample(False, 0.15, 81).collect() 						# Return sampled subset of rdd3
[3,4,27,31,40,41,42,43,60,76,79,80,86,97]

Filtering:
>>> rdd.filter(lambda x: "a" in x).collect()
	[('a',7),('a',2)]
>>> rdd5.distinct().collect()
   	['a',2,'b',7]
>>> rdd.keys().collect()
	['a', 'a', 'b']

Iterating:
=========
Apply a function to all RDD elements
>>> def g(x): print(x)
>>> rdd.foreach(g)
 ('a', 7)
 ('b', 2)
 ('a', 2)

Reshaping Data:
==============
Reducing:
>>> rdd.reduceByKey(lambda x,y : x+y).collect()						# Merge the rdd values for each key
  [('a',9),('b',2)]
>>> rdd.reduce(lambda a, b: a + b)							# Merge the rdd values
  ('a',7,'a',2,'b',2)

Grouping by:
>>> rdd3.groupBy(lambda x: x % 2)							# Return RDD of grouped values
        .mapValues(list)
        .collect()
>>> rdd.groupByKey()									# Group rdd by key
       .mapValues(list)
       .collect()
[('a',[7,2]),('b',[2])]

Aggregating:
>>> seqOp = (lambda x,y: (x[0]+y,x[1]+1)) 						# Aggregate RDD elements of each partition and then the results Aggregate values of each RDD key
>>> combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) 
>>> rdd3.aggregate((0,0),seqOp,combOp)
	(4950,100)
>>> rdd.aggregateByKey((0,0),seqop,combop).collect()					# Aggregate the elements of each partition, and then the results Merge the values for each key
  	[('a',(9,2)), ('b',(2,1))]
>>> rdd3.fold(0,add)
	4950
>>> rdd.foldByKey(0, add).collect()
  	[('a',9),('b',2)]
>>> rdd3.keyBy(lambda x: x+x).collect()							# Create tuples of RDD elements by applying a function

Mathematical Operations:
=======================
>> rdd.subtract(rdd2).collect()								# Return each rdd value not contained in rdd2
  	[('b',2),('a',7)]
>>> rdd2.subtractByKey(rdd).collect()							# Return each (key,value) pair of rdd2 with no matching key in rdd
	[('d', 1)]
>>> rdd.cartesian(rdd2).collect()							# Return the Cartesian product of rdd and rdd2

Sort:
====
>>> rdd2.sortBy(lambda x: x[1]).collect()						# Sort RDD by given function
  	[('d',1),('b',1),('a',2)]
>>> rdd2.sortByKey().collect()								# Sort (key, value) RDD by key
	[('a',2),('b',1),('d',1)]

Repartitioning:
==============
>>> rdd.repartition(4)									# New RDD with 4 partitions
>>> rdd.coalesce(1)									# Decrease the number of partitions in the RDD to 1

Saving:
======
>>> rdd.saveAsTextFile("rdd.txt")
>>> rdd.saveAsHadoopFile("hdfs://namenodehost/parent/child",'org.apache.hadoop.mapred.TextOutputFormat')

>>> sc.stop()										=> Stopping SparkContext => 
$ ./bin/spark-submit examples/src/main/python/pi.py					=> Execution

========================================================== pandas elaborated with examples =========================================
https://www.geeksforgeeks.org/python-pandas-dataframe/

Python | Pandas DataFrame
Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns).
Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects.

Example 1: Creating a DataFrame using List.

import pandas as pd		# import pandas library as pd
data = ['Geeks','For','Geeks']	# creating list
df = pd.DataFrame(data)		# creating dataframe using DataFrame object

print(df)
Output:

       0
0  Geeks
1    For
2  Geeks
 

Example 2: Creating a DataFrame using Arrays

import pandas as pd
data = {'A':['Geeks', 'For', 'Geeks'],'B':['Welcome',2,'Geeks']}
df = pd.DataFrame(data, index = ['One','Two','Three'])
 
print (df)
Output:

           A        B
One    Geeks  Welcome
Two      For        2
Three  Geeks    Geeks

Python | Pandas Series
Pandas Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.
Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index.

Example 1: Create a basic Series

import pandas as pd
ser = pd.Series()			# create empty series
print(ser)
 
ser = pd.Series([1, 2, 3, 4, 5])	# create series form a list
print(ser)
Output:

Series([], dtype: float64)

0   1
1   2
2   3
3   4
4   5
dtype: int64

Example 2: Create a Series using Dictionary

import pandas as pd
 
dict = {'Geeks' : 10,
        'for' : 20,
        'geeks' : 30}
 
ser = pd.Series(dict)
 
print(ser)
Output:

Geeks    10
for      20
geeks    30
dtype: int64

Python | Pandas Working With Text Data
Series and Indexes are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods.

Example 1: Use of Str.lower() method

import pandas as pd 
   
data = pd.read_csv("employees.csv") 			# making data frame from csv file 
data["First Name"]= data["First Name"].str.lower() 	# converting and overwriting values in column 
data 							# display 
 
Example 2 : Use of str.find() method

import pandas as pd 
   
data = pd.read_csv("https://cdncontribute.geeksforgeeks.org/wp-content/uploads/nba.csv") 					# reading csv file from url
data.dropna(inplace = True) 													# dropping null value columns to avoid errors
sub ='a'															# substring to be searched 
   
# creating and passsing series to new column 
data["Indexes"]= data["Name"].str.find(sub) 
   
# display 
data 

Python | Pandas Working with Dates and Times
Pandas has proven very successful as a tool for working with time series data, especially in the financial data analysis space. Using the NumPy datetime64 and timedelta64 dtypes, we have consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data.

Example #1: Create a dates dataframe

import pandas as pd
 
# Create dates dataframe with frequency  
data = pd.date_range('1/1/2011', periods = 10, freq ='H')
 
data
Output:
DatetimeIndex(['2011-01-01 00:00:00', '2011-01-01 01:00:00',
               '2011-01-01 02:00:00', '2011-01-01 03:00:00',
               '2011-01-01 04:00:00', '2011-01-01 05:00:00',
               '2011-01-01 06:00:00', '2011-01-01 07:00:00',
               '2011-01-01 08:00:00', '2011-01-01 09:00:00'],
              dtype='datetime64[ns]', freq='H')


Example #2: Create range of dates and show basic features

# Create date and time with dataframe
data = pd.date_range('1/1/2011', periods = 10, freq ='H')
 
x = datetime.now()
x.month, x.year
Output:

(9, 2018)
 
Example #3: Break data and time into seperate features

# Create date and time with dataframe
rng = pd.DataFrame()
rng['date'] = pd.date_range('1/1/2011', periods = 72, freq ='H')
 
# Print the dates in dd-mm-yy format
rng[:5]
 
# Create features for year, month, day, hour, and minute
rng['year'] = rng['date'].dt.year
rng['month'] = rng['date'].dt.month
rng['day'] = rng['date'].dt.day
rng['hour'] = rng['date'].dt.hour
rng['minute'] = rng['date'].dt.minute
 
# Print the dates divided into features
rng.head(3)
Output:

Python | Pandas Merging, Joining, and Concatenating
Pandas provide various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.

Merge, Join and Concatenate DataFrames using Panda

Example #1 : DataFrames Concatenation

concat() function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.

# Python program to concatenate
# dataframes using Panda
 
# Creating first dataframe
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3'],
                    'C': ['C0', 'C1', 'C2', 'C3'],
                    'D': ['D0', 'D1', 'D2', 'D3']},
                    index = [0, 1, 2, 3])
 
# Creating second dataframe
df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
                    'B': ['B4', 'B5', 'B6', 'B7'],
                    'C': ['C4', 'C5', 'C6', 'C7'],
                    'D': ['D4', 'D5', 'D6', 'D7']},
                    index = [4, 5, 6, 7])
 
# Creating third dataframe
df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],
                    'B': ['B8', 'B9', 'B10', 'B11'],
                    'C': ['C8', 'C9', 'C10', 'C11'],
                    'D': ['D8', 'D9', 'D10', 'D11']},
                    index = [8, 9, 10, 11])
 
# Concatenating the dataframes
pd.concat([df1, df2, df3])
Output:
Concatenation

 
Example #2 : DataFrames Merge

Pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame objects.

# Python program to merge
# dataframes using Panda
 
# Dataframe created
left = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'],
                    'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']})
 
right = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'],
                      'C': ['C0', 'C1', 'C2', 'C3'],
                      'D': ['D0', 'D1', 'D2', 'D3']})
                       
# Merging the dataframes                      
pd.merge(left, right, how ='inner', on ='Key')
Output:
Merging
 
Code #3 : DataFrames Join

# Python program to join
# dataframes using Panda
 
left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']},
                    index = ['K0', 'K1', 'K2', 'K3'])
 
right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],
                      'D': ['D0', 'D1', 'D2', 'D3']},
                      index = ['K0', 'K1', 'K2', 'K3'])
                       
# Joining the dataframes                      
left.join(right)
Output:

Joining

Python | Data analysis using Pandas

Pandas is the most popular python library that is used for data analysis. It provides highly optimized performance with back-end source code is purely written in C or Python.

We can analyze data in pandas with:

Series
DataFrames
Series:

Series is one dimensional(1-D) array defined in pandas that can be used to store any data type.

Code #1: Creating Series

filter_none
edit
play_arrow
brightness_4
# Program to create series 
import pandas as pd  # Import Panda Library 
  
# Create series with Data, and Index 
a = pd.Series(Data, index = Index)   
Here, Data can be:

A Scalar value which can be integerValue, string
A Python Dictionary which can be Key, Value pair
A Ndarray
Note: Index by default is from 0, 1, 2, …(n-1) where n is length of data.
 
Code #2: When Data contains scalar values



 

filter_none
edit
play_arrow
brightness_4
# Program to Create series with scalar values  
Data =[1, 3, 4, 5, 6, 2, 9]  # Numeric data 
  
# Creating series with default index values 
s = pd.Series(Data)     
  
# predefined index values 
Index =['a', 'b', 'c', 'd', 'e', 'f', 'g']  
  
# Creating series with predefined index values 
si = pd.Series(Data, Index)  
Output:
>>> print si
a    1
b    3
c    4
d    5
e    6
f    2
g    9

Scalar Data with default Index



Scalar Data with Index


 
Code #3: When Data contains Dictionary

filter_none
edit
play_arrow
brightness_4
# Program to Create Dictionary series 
dictionary ={'a':1, 'b':2, 'c':3, 'd':4, 'e':5}  
  
# Creating series of Dictionary type 
sd = pd.Series(dictionary)  
Output:


Dictionary type data


 

Code #4:When Data contains Ndarray

filter_none
edit
play_arrow
brightness_4
# Program to Create ndarray series 
Data =[[2, 3, 4], [5, 6, 7]]  # Defining 2darray 
  
# Creating series of 2darray 
snd = pd.Series(Data)     
Output:


Data as Ndarray


 

DataFrames:

DataFrames is two-dimensional(2-D) data structure defined in pandas which consists of rows and columns.

Code #1: Creation of DataFrame

filter_none
edit
play_arrow
brightness_4
# Program to Create DataFrame 
import pandas as pd   # Import Library 
  
a = pd.DataFrame(Data)  # Create DataFrame with Data 
Here, Data can be:



 

One or more dictionaries
One or more Series
2D-numpy Ndarray
 
Code #2: When Data is Dictionaries

filter_none
edit
play_arrow
brightness_4
# Program to Create Data Frame with two dictionaries 
dict1 ={'a':1, 'b':2, 'c':3, 'd':4}        # Define Dictionary 1 
dict2 ={'a':5, 'b':6, 'c':7, 'd':8, 'e':9} # Define Dictionary 2 
Data = {'first':dict1, 'second':dict2}  # Define Data with dict1 and dict2 
df = pd.DataFrame(Data)  # Create DataFrame 
Output:


DataFrame with two dictionaries


 
Code #3: When Data is Series

filter_none
edit
play_arrow
brightness_4
# Program to create Dataframe of three series  
import pandas as pd 
  
s1 = pd.Series([1, 3, 4, 5, 6, 2, 9])           # Define series 1 
s2 = pd.Series([1.1, 3.5, 4.7, 5.8, 2.9, 9.3]) # Define series 2 
s3 = pd.Series(['a', 'b', 'c', 'd', 'e'])     # Define series 3 
  
  
Data ={'first':s1, 'second':s2, 'third':s3} # Define Data 
dfseries = pd.DataFrame(Data)              # Create DataFrame 
Output:


DataFrame with three series


 
Code #4: When Data is 2D-numpy ndarray
Note: One constraint has to be maintained while creating DataFrame of 2D arrays – Dimensions of 2D array must be same.

filter_none
edit
play_arrow
brightness_4
# Program to create DataFrame from 2D array 
import pandas as pd # Import Library 
d1 =[[2, 3, 4], [5, 6, 7]] # Define 2d array 1 
d2 =[[2, 4, 8], [1, 3, 9]] # Define 2d array 2 
Data ={'first': d1, 'second': d2} # Define Data  
df2d = pd.DataFrame(Data)    # Create DataFrame 
Output:


DataFrame with 2d ndarray
https://www.geeksforgeeks.org/python-data-analysis-using-pandas/
Python | Read csv using pandas.read_csv()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Import Pandas:

import pandas as pd
 
Code #1 : read_csv is an important pandas function to read csv files and do operations on it.

filter_none
brightness_4
# Import pandas 
import pandas as pd 
  
# reading csv file  
pd.read_csv("filename.csv") 
Opening a CSV file through this is easy. But there are many others thing one can do through this function only to change the returned object completely. For instance, one can read a csv file not only locally, but from a URL through read_csv or one can choose what columns needed to export so that we don’t have to edit the array later.

Here is the list of parameters it takes with their Default values.



 

pd.read_csv(filepath_or_buffer, sep=’, ‘, delimiter=None, header=’infer’, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=’infer’, thousands=None, decimal=b’.’, lineterminator=None, quotechar='”‘, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)
Not all of them are much important but remembering these actually save time of performing same functions on own. One can see parameters of any function by pressing shift + tab in jupyter notebook. Useful ones are given below with their usage :

PARAMETER	USE
filepath_or_buffer	URL or Dir location of file
sep	Stands for seperator, default is ‘, ‘ as in csv(comma seperated values)
index_col	Makes passed column as index instead of 0, 1, 2, 3…r

header	Makes passed row/s[int/int list] as header


use_cols	Only uses the passed col[string list] to make data frame
squeeze	If true and only one column is passed, returns pandas series
skiprows	Skips passed rows in new data frame
Refer the link to data set used from here.

Code #2 :

filter_none
brightness_4
# importing Pandas library 
import pandas as pd 
  
pd.read_csv(filepath_or_buffer = "pokemon.csv") 
  
# makes the passed rows header 
pd.read_csv("pokemon.csv", header =[1, 2]) 
  
# make the passed column as index instead of 0, 1, 2, 3.... 
pd.read_csv("pokemon.csv", index_col ='Type') 
  
# uses passed cols only for data frame 
pd.read_csv("pokemon.csv", usecols =["Type"]) 
  
# reutruns pandas series if there is only one colunmn 
pd.read_csv("pokemon.csv", usecols =["Type"], 
                              squeeze = True) 
                                
# skips the passed rows in new series 
pd.read_csv("pokemon.csv", 
            skiprows = [1, 2, 3, 4]) 

Python | Merge, Join and Concatenate DataFrames using Panda

A dataframe is a two-dimensional data structure having multiple rows and columns. In a dataframe, the data is aligned in the form of rows and columns only. A dataframe can perform arithmetic as well as conditional operations. It has mutable size.

Below is the implementation using Numpy and Pandas.

Modules needed:

import numpy as np
import pandas as pd
 

Code #1 : DataFrames Concatenation
concat() function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.

 

filter_none
brightness_4
# Python program to concatenate 
# dataframes using Panda 
  
# Creating first dataframe 
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3'], 
                    'C': ['C0', 'C1', 'C2', 'C3'], 
                    'D': ['D0', 'D1', 'D2', 'D3']}, 
                    index = [0, 1, 2, 3]) 
  
# Creating second dataframe 
df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 
                    'B': ['B4', 'B5', 'B6', 'B7'], 
                    'C': ['C4', 'C5', 'C6', 'C7'], 
                    'D': ['D4', 'D5', 'D6', 'D7']}, 
                    index = [4, 5, 6, 7]) 
  
# Creating third dataframe 
df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'], 
                    'B': ['B8', 'B9', 'B10', 'B11'], 
                    'C': ['C8', 'C9', 'C10', 'C11'], 
                    'D': ['D8', 'D9', 'D10', 'D11']}, 
                    index = [8, 9, 10, 11]) 
  
# Concatenating the dataframes 
pd.concat([df1, df2, df3]) 
Output:
Concatenation

 
Code #2 : DataFrames Merge
Pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame objects.

filter_none
brightness_4
# Python program to merge 
# dataframes using Panda 
  
# Dataframe created 
left = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'], 
                    'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3']}) 
  
right = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'], 
                      'C': ['C0', 'C1', 'C2', 'C3'], 
                      'D': ['D0', 'D1', 'D2', 'D3']}) 
                        
# Merging the dataframes                       
pd.merge(left, right, how ='inner', on ='Key') 
Output:
Merging
 
Code #3 : DataFrames Join

filter_none
brightness_4
# Python program to join 
# dataframes using Panda 
  
left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3']}, 
                    index = ['K0', 'K1', 'K2', 'K3']) 
  
right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'], 
                      'D': ['D0', 'D1', 'D2', 'D3']}, 
                      index = ['K0', 'K1', 'K2', 'K3']) 
                        
# Joining the dataframes                       
left.join(right) 
Output:

Joining

Python | Delete rows/columns from DataFrame using Pandas.drop()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Pandas provide data analysts a way to delete and filter data frame using .drop() method. Rows or columns can be removed using index label or column name using this method.

Syntax:
DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’)

Parameters:

labels: String or list of strings referring row or column name.
axis: int or string value, 0 ‘index’ for Rows and 1 ‘columns’ for Columns.
index or columns: Single label or list. index or columns are an alternative to axis and cannot be used together.
level: Used to specify level in case data frame is having multiple level index.
inplace: Makes changes in original Data Frame if True.
errors: Ignores error if any value from the list doesn’t exists and drops rest of the values when errors = ‘ignore’



 

Return type: Dataframe with dropped values
To download the CSV used in code, click here.
Example #1: Dropping Rows by index label
In his code, A list of index labels is passed and the rows corresponding to those labels are dropped using .drop() method.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed values 
data.drop(["Avery Bradley", "John Holland", "R.J. Hunter", 
                            "R.J. Hunter"], inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed values. Those values were dropped and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping values-


Data Frame after Dropping values-


 

Example #2 : Dropping columns with column name

In his code, Passed columns are dropped using column names. axis parameter is kept 1 since 1 refers to columns.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed columns 
data.drop(["Team", "Weight"], axis = 1, inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed columns. Those values were dropped since axis was set equal to 1 and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping Columns-


Data Frame after Dropping Columns-

Python | Delete rows/columns from DataFrame using Pandas.drop()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Pandas provide data analysts a way to delete and filter data frame using .drop() method. Rows or columns can be removed using index label or column name using this method.

Syntax:
DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’)

Parameters:

labels: String or list of strings referring row or column name.
axis: int or string value, 0 ‘index’ for Rows and 1 ‘columns’ for Columns.
index or columns: Single label or list. index or columns are an alternative to axis and cannot be used together.
level: Used to specify level in case data frame is having multiple level index.
inplace: Makes changes in original Data Frame if True.
errors: Ignores error if any value from the list doesn’t exists and drops rest of the values when errors = ‘ignore’



 

Return type: Dataframe with dropped values
To download the CSV used in code, click here.
Example #1: Dropping Rows by index label
In his code, A list of index labels is passed and the rows corresponding to those labels are dropped using .drop() method.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed values 
data.drop(["Avery Bradley", "John Holland", "R.J. Hunter", 
                            "R.J. Hunter"], inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed values. Those values were dropped and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping values-


Data Frame after Dropping values-


 

Example #2 : Dropping columns with column name

In his code, Passed columns are dropped using column names. axis parameter is kept 1 since 1 refers to columns.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed columns 
data.drop(["Team", "Weight"], axis = 1, inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed columns. Those values were dropped since axis was set equal to 1 and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping Columns-


Data Frame after Dropping Columns-

Python | Data Comparison and Selection in Pandas

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages, and makes importing and analyzing data much easier.

The most important thing in Data Analysis is comparing values and selecting data accordingly. The “==” operator works for multiple values in a Pandas Data frame too. Following two examples will show how to compare and select data from a Pandas Data frame.

To download the CSV file used, Click Here.

Example #1: Comparing Data
In the following example, a data frame is made from a csv file. In the Gender Column, there are only 3 types of values (“Male”, “Female” or NaN). Every row of Gender column is compared to “Male” and a boolean series is returned after that.

filter_none
brightness_4
# importing pandas package 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("employees.csv") 
  
# storing boolean series in new 
new = data["Gender"] == "Male"
  
# inserting new series in data frame 
data["New"]= new 
  
# display 
data 
Output:
As show in the output image, for Gender= “Male”, the value in New Column is True and for “Female” and NaN values it is False.



 


 
Example #2: Selecting Data
In the following example, the boolean series is passed to the data and only Rows having Gender=”Male” are returned.

filter_none
brightness_4
# importing pandas package 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("employees.csv") 
  
# storing boolean series in new 
new = data["Gender"] != "Female"
  
# inserting new series in data frame 
data["New"]= new 
  
# display 
data[new] 
  
# OR  
# data[data["Gender"]=="Male"] 
# Both are the same 
Output:
As shown in the output image, Data frame having Gender=”Male” is returned.



pandas.pydata			(This is vast; needs to be learned separately)
=============
df.expanding()			- Return an Expanding object allowing summary functions to be applied cumulatively.
df.rolling(n)			- Return a Rolling object allowing summary functions to be applied to windows of length n.

==================================== Apache Spark : RDD vs DataFrame vs Dataset ===========================================================

With Spark2.0 release, there are 3 types of data abstractions which Spark officially provides now to use : RDD,DataFrame and DataSet


Short Combined Intro :
Evolution of these abstractions happened in this way :

RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6)					#RDD being the oldest available from 1.0 version to Dataset being the newest available from 1.6 version.

Given same data, each of the 3 abstraction will compute and give same results to user. But they differ in performance and the ways they compute.

RDD lets us decide HOW we want to do which limits the optimisation Spark can do on processing underneath where as dataframe/dataset lets us decide WHAT we want to do and leave everything on Spark to decide how to do computation.

We will understand this in 2 minutes what is meant by HOW & WHAT

Dataframe came as a major performance improvement over RDD but not without some downsides.

This led to development of Dataset which is an effort to unify best of RDD and data frame.

In future, Dataset will eventually replace RDD and Dataframe to become the only API spark users should be using in code.

Lets understand them in detail one by one.

RDD:
-Its building block of spark. No matter which abstraction Dataframe or Dataset we use, internally final computation is done on RDDs.
-RDD is lazily evaluated immutable parallel collection of objects exposed with lambda functions.
-The best part about RDD is that it is simple. It provides familiar OOPs style APIs with compile time safety. We can load any data from a source,convert them into RDD and store in memory to compute results. RDD can be easily cached 
if same set of data needs to recomputed.

-But the disadvantage is performance limitations. Being in-memory jvm objects, RDDs involve overhead of Garbage Collection and Java(or little better Kryo) Serialisation which are expensive when data grows.

Dataframe:
-DataFrame is an abstraction which gives a schema view of data. Which means it gives us a view of data as columns with column name and types info, We can think data in data frame like a table in database.
-Like RDD, execution in Dataframe too is lazy triggered .
-offers huge performance improvement over RDDs because of 2 powerful features it has:
	1. Custom Memory management (aka Project Tungsten)
		Data is stored in off-heap memory in binary format. This saves a lot of memory space. Also there is no Garbage Collection overhead involved. By knowing the schema of data in advance and storing efficiently in 
		binary format, expensive java Serialization is also avoided.

	2. Optimized Execution Plans (aka Catalyst Optimizer)
		Query plans are created for execution using Spark catalyst optimiser. After an optimised execution plan is prepared going through some steps, the final execution happens internally on RDDs only but thats completely 
		hidden from the users.

Dataframe Limitations:-
Compile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not known. However, you will get a Runtime exception when executing this code.
Example:

case class Person(name : String , age : Int) 
val dataframe = sqlContect.read.json("people.json") 
dataframe.filter("salary > 10000").show 
=> throws Exception : cannot resolve 'salary' given input age , name
This is challenging specially when you are working with several transformation and aggregation steps.

Cannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be 
recover the original RDD of Person class (RDD[Person]).
Example:

case class Person(name : String , age : Int)
val personRDD = sc.makeRDD(Seq(Person("A",10),Person("B",20)))
val personDF = sqlContext.createDataframe(personRDD)
personDF.rdd // returns RDD[Row] , does not returns RDD[Person]

Dataframe example:
2 ways to define: 1. Expression BuilderStyle 2. SQL Style

As discussed, If we try using some columns not present in schema, we will get problem only at runtime . For example, if we try accessing salary when only name and age are present in the schema will throw AnalysisException


Dataset Features:-
Provides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)
Encoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.
Programming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.
Type Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.
-It is an extension to Dataframe API, the latest abstraction which tries to provide best of both RDD and Dataframe.

-comes with OOPs style and developer friendly compile time safety like RDD as well as performance boosting features of Dataframe : Catalyst optimiser and custom memory management.
-How dataset scores over Dataframe is an additional feature it has: Encoders .
-Encoders act as interface between JVM objects and off-heap custom memory binary format data.

-Encoders generate byte code to interact with off-heap data and provide on-demand access to individual attributes without having to de-serialize an entire object.

-case class is used to define the structure of data schema in Dataset. Using case class, its very easy to work with dataset. Names of different attributes in case class is directly mapped to attributes in Dataset . It gives feeling 
like working with RDD but actually underneath it works same as Dataframe.

Dataframe is infact treated as dataset of generic row objects. DataFrame=Dataset[Row] . So we can always convert a data frame at any point of time into a dataset by calling ‘as’ method on Dataframe.
 e.g. http://df.as[MyClass]

Important point to remember is that both Dataset and DataFrame internally does final execution on RDD objects only but the difference is users do not write code to create the RDD collections and have no control as such over RDDs. RDDs are created in the execution plan as last stage after deciding and going through all the optimizations (see Execution Plan Diagram).

And all these optimisations could have been possible because data is structured and Spark knows about the schema of data in advance. So it can apply all the powerful features like tungsten custom memory off-heap binary storage,catalyst 
optimiser and encoders to get the performance which was not possible if users would have been directly working on RDD.

Conclusion:

In short, Spark is moving from unstructured computation(RDDs) towards structured computation because of many performance optimisations it allows . Data frame was a step in direction of structured computation but lacked developer friendliness of compile time safety,lambda functions. Finally Dataset is the unification of Dataframe and RDD to bring the best abstraction out of two.

Going forward developers should only be concerned about DataSet while Dataframe and RDD will be discouraged to use. But its always better to be aware of the legacy for better understanding of internals.

Interestingly,most of these new concepts like custom memory management(tungsten),logical/physical plans(catalyst optimizer),encoders(dataset),etc seems to be inspired from its competitor Apache Flink which inherently supports these since inception.There are other new powerful feature enhancements like windowing,sessions,etc coming in Spark which are already in Flink. So its better to keep a close watch on both Spark and Flink in coming days.

==================================== Sparkx.py  ==============================================================

from common_base.base_test import ReactorXTestBase
import os, sys, json

class SparkX(ReactorXTestBase):

    # Constructor
    def __init__(self):

        batchx_path = self.get_value('batchx_path')

        SPARK_HOME = os.path.join(batchx_path, 'app/spark-2.1.1-bin-hadoop2.6/')
        self.assertTrue(
            os.path.exists(SPARK_HOME),
            'Make sure that SPARK_HOME[{}] exists and is a valid path.'.format(SPARK_HOME)
        )

        # Add the PySpark directories to the Python path:
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python'))
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python', 'pyspark'))
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python', 'build'))

        # This is to put a check if the version of py4j changes in future
        py4j_src_path = os.path.join(SPARK_HOME, 'python', 'lib/py4j-0.10.4-src.zip')
        self.assertTrue(
            os.path.exists(py4j_src_path),
            'Make sure that py4j_src_path[{}] exists and is a valid path.'.format(py4j_src_path)
        )

        sys.path.insert(1, py4j_src_path)
        # print SPARK_HOME

        from pyspark.conf import SparkConf
        from pyspark.sql import SparkSession, SQLContext, Row
        from pyspark.sql.types import StringType, IntegerType, StructType, StructField

        """Create a single node Spark application."""
        sc_conf = SparkConf()
        sc_conf.setAppName("batchx_tests")
        sc_conf.setMaster('local[2]')
        sc_conf.set('spark.executor.memory', '2g')
        sc_conf.set('spark.executor.cores', '4')
        sc_conf.set('spark.cores.max', '40')
        sc_conf.set('spark.logConf', True)
        # print sc_conf.getAll()

        SparkSession._instantiatedContext = None
        self.spark = SparkSession.builder.config(conf=sc_conf).getOrCreate()
        self.sparkSession = SparkSession

        sc = None
        try:
            sc.stop()
            sc = self.spark.sparkContext
        except:
            sc = self.spark.sparkContext

        sc.setLogLevel("ERROR")

        self.sqlContext = SQLContext(sc)
        self.row = Row
        self.sc = sc
        self.stringtype = StringType
        self.structtype = StructType
        self.structfield = StructField

    def convert_data_to_dataframe(self, data, table_name='data_table'):
        '''
        Converts a list or any other data type to dataframe
        :param data:
        :param table_name:
        :return:
        '''
        schematype = None
        if type(data) is str:
            schematype = self.stringtype

        # Update the method is you know the schematype for list or dictionary or else keep it as None.
        # e.g. if type(data) is list: . . .

        df = self.spark.createDataFrame(data, schematype)
        df.createOrReplaceTempView(table_name)
        return df

    def readDBF(self, dbf_file):
        '''
        read .dbf file
        :param dbf_file:
        :return:
        '''
        from dbfread import DBF
        # print len(DBF(dbf_file))
        # if i use "load = true" to load the file first, the search is faster, but it takes a lot of memory.
        return DBF(dbf_file, load=True)

        """for record in DBF(dbf_file, load=True):
            print record
            for key, value in record.items():
                try:
                    value = str(value)
                except:
                    value = value.encode('utf-8')"""

    def convert_shape_file_to_geojson(self, shp_path):
        '''
        Convert shape file to geojson
        :param shp_path:
        :return:
        '''

        import io
        import shapefile

        blocks = self.sc.binaryFiles(shp_path)
        block_dict = dict(blocks.collect())

        reader = shapefile.Reader(
            shp=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".shp")][0]]),
            shx=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".shx")][0]]),
            dbf=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".dbf")][0]])
        )

        field_names = [x[0] for x in reader.fields][1:]
        buffer = []
        for sr in reader.shapeRecords():
            atr = dict(zip(field_names, sr.record))
            geom = sr.shape.__geo_interface__
            buffer.append(
                dict(type="Feature",
                     geometry=geom,
                     properties=atr
                     )
            )

        data = json.dumps({"type": "FeatureCollection", "features": buffer}, indent=2)
        self.sc.parallelize(data)

        batchx_path = self.get_value('batchx_path')
        with open(os.path.join(batchx_path, 'urban_area.geojson'), "w") as geojson:
            geojson.write(str(data))

        '''
        # As the data is too large currently writing it to a geojson file and reading it later.
        # I'll find out a better way of implementing this
        with open(os.path.join(batchx_path, 'urban_area.geojson'), "w") as geojson:
            geojson.write(data)'''

    def read_suggested_geojson(self, geojson_file='data/outputUrbanRural/suggestedGrid.geojson'):
        '''
        read suggested geojson
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        file_path = os.path.join(batchx_path, geojson_file)
        return self.read_json_file_return_rdd(file_path)

    def read_json_file_return_rdd(self, file_path):
        '''
        read json return rdd
        :param file_path:
        :return:
        '''
        data = None
        with open(file_path, 'r') as fh:
            data = fh.read()
        json_data = json.loads(data)
        all_coordinates = []
        for feature in json_data['features']:
            for coordinates in feature['geometry']['coordinates'][0]:
                all_coordinates.append(coordinates)

        rdd = self.sc.parallelize(all_coordinates)
        return rdd

    def validate_todo_filter_type_existence(self, todo_type_filter, rel_todo_file_path):
        '''
        validate todo filter type exists or not
        :param todo_type_filter:
        :param rel_todo_file_path:
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        todo_file_path = os.path.join(batchx_path, rel_todo_file_path)

        table_name = self.get_todo_items_rdd_table(todo_file_path)

        data = self.sqlContext.sql(
            "SELECT todo_id FROM " + table_name + " WHERE item.type == \'" + todo_type_filter + "\'"
        )
        '''For testing'''
        # data.show()
        # data = self.sqlContext.sql("SELECT todo_id FROM " + table_name + " WHERE item.type == 'MANEUVER_RESTRICTION'")

        todo_ids = data.rdd.map(lambda x: str(x[0])).collect()
        self.assertTrue(
            len(todo_ids) == 0,
            'Failed to apply filter type[{}] for todo_ids [{}]'.format(todo_type_filter, str(todo_ids))
        )

    def get_todo_items_rdd_table(self, todo_file_path):
        '''
        :param todo_file_path:
        :return:
        '''
        df = self.sqlContext.read.json(todo_file_path)
        # for testing
        # print df.first().__getitem__('item').__getitem__('type')
        # df.select('item').show()
        # df.select('item').select('type').show()
        # df.printSchema()
        table_name = "todo_table"
        df.createOrReplaceTempView(table_name)
        return table_name

    def convert_rdd_to_dataframe(self, user_rdd, field_name):
        '''
        :param user_rdd: rdd
        :param field_name: feature_id
        :return:
        '''
        schema = self.structtype([self.structfield(field_name, self.stringtype(), True)])
        user_df = self.sqlContext.createDataFrame(user_rdd.map(lambda x: (x,)), schema)
        return user_df.distinct()

    def get_dataframe(self, khRepo='data/khRepo/', table_name='khRepo_table'):
        '''
        Read data/khRepo and return dataframe
        :param khRepo:
        :param table_name:
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        dataFrame = self.spark.read.json(os.path.join(batchx_path, khRepo))  # time taken =  10.959002018
        dataFrame.createOrReplaceTempView(table_name)  # Create a temporary view using the DataFrame
        return dataFrame

    # Destructor
    def __del__(self):
        try:
            self.sc.stop()
        except:
            self.logger.info('Spark context is not set yet.')

================================================================= Calling function ==================================
        sparkx = SparkX()

        df = sparkx.get_dataframe('data/khRepo/')
        self.logger.info(
            "{}: input features count: {}. get downloaded features".format(
                test_name, df.count()
            )
        )
        df.show()

        df_tool_output_featureproto = sparkx.get_dataframe('data/output/tool_output.featureproto')
        self.logger.info(
            "{}: output features count: {}. Reading tool_output.featureproto".format(
                test_name, df_tool_output_featureproto.count()
            )
        )

        df_tool_output_featureproto.show()  # just to print on console

        df_actual_features = df.join(
            df_tool_output_featureproto,
            df.feature_id == df_tool_output_featureproto.feature_id
        ).select(
            df_tool_output_featureproto.FeatureProto
        ).where(df.FeatureProto.feature_type[0] == 'ROAD_SEGMENT')

        df_actual_features.show()
============================================================== Calling function 2 =====================================
        sparkx = SparkX()

        # Make sure that expected number of buildings feature ids are downloaded in building-footprints.json
        df_building_footprints = sparkx.get_dataframe('data/buildingFiles/building-footprints/building-footprints.json')
        actual_building_footprints = df_building_footprints.count()

        # Make sure that expected number of 3DV building feature ids are converted to NDM
        df_tool_output = sparkx.get_dataframe('data/output/tool_output.featureproto')
        tool_output_feature_count = df_tool_output.count()

        # Make sure that all the 3DV building footprints are converted to NDM
        # After conversion to NDM features should have build_id starting with string "NeutronBatchX /  reactorx-apps-"
        three_dv_to_ndm_features = df_building_footprints.join(
            df_tool_output
        ).select(
            df_tool_output.metadata.source_attribution.sub_path_source[0].process_info.build_id,
            # Fields below have unique values i.e. the values are not same 3DV buildings and NDM featureProto
            df_tool_output.FeatureProto.is_active,  # is_active is not in buildin_footprints
            df_tool_output.FeatureProto.protocol_version,
            df_building_footprints.FeatureProto.protocol_version,
            df_tool_output.FeatureProto.version,
            df_building_footprints.FeatureProto.version,
            df_tool_output.FeatureProto.representative_point,
            df_building_footprints.FeatureProto.representative_point
        ).withColumnRenamed(  # Rename the column as it's too big
            'metadata.source_attribution.sub_path_source[0].process_info.build_id', 'build_id'
        ).where(  # Verifying if there is any mismatch in any of the below fields
            (df_tool_output.FeatureProto.feature_type[0] == 'BUILDING') &
            (df_tool_output.FeatureProto.feature_id == df_building_footprints.FeatureProto.feature_id) &
            (df_tool_output.FeatureProto.building.roof_type == df_building_footprints.FeatureProto.building.roof_type) &
            (
                    df_tool_output.FeatureProto.building.base_height == df_building_footprints.FeatureProto.building.base_height) &
            (
                    df_tool_output.FeatureProto.building.top_height == df_building_footprints.FeatureProto.building.top_height) &
            (df_tool_output.FeatureProto.iso_country_code == df_building_footprints.FeatureProto.iso_country_code) &
            (
                    df_tool_output.FeatureProto.iso_subdivision_code == df_building_footprints.FeatureProto.iso_subdivision_code) &
            (df_tool_output.FeatureProto.history[0].edit_type == df_building_footprints.FeatureProto.history[
                0].edit_type) &
            (df_tool_output.FeatureProto.history[0].source == df_building_footprints.FeatureProto.history[0].source) &
            (df_tool_output.FeatureProto.history[0].source_id == df_building_footprints.FeatureProto.history[
                0].source_id) &
            (df_tool_output.FeatureProto.history[0].timestamp == df_building_footprints.FeatureProto.history[
                0].timestamp) &
            (df_tool_output.FeatureProto.history[0].vendor_id == df_building_footprints.FeatureProto.history[
                0].vendor_id)
        ).where(
            "build_id like '%NeutronBatchX /  reactorx-apps-%'"
        )
        three_dv_to_ndm_features.show()  # Just for reference. It prints the table

        three_dv_to_ndm_features_count = three_dv_to_ndm_features.count()

======================================================== Spark Q&A ==============
Question 4. What Is Rdd?
Answer :
RDDs (Resilient Distributed Datasets) are basic abstraction in Apache Spark that represent the data coming into the system in object format. RDDs are used for in-memory computations on large clusters, in a fault tolerant manner. RDDs are read-only portioned, collection of records, that are –
Immutable – RDDs cannot be altered.
Resilient – If a node holding the partition fails the other node takes the data.

Question 1. What Is Shark?
Answer :
Most of the data users know only SQL and are not good at programming. Shark is a tool, developed for people who are from a database background - to access Scala MLib capabilities through Hive like SQL interface. Shark tool helps data users run Hive on Spark - offering compatibility with Hive metastore, queries and data.

Sensor Data Processing –Apache Spark’s ‘In-memory computing’ works best here, as data is retrieved and combined from different sources.
Spark is preferred over Hadoop for real time querying of data
Stream Processing – For processing logs and detecting frauds in live streams for alerts, Apache Spark is the best solution.

Question 3. What Is A Sparse Vector?
Answer :
sparse vector has two parallel arrays –one for indices and the other for values. These vectors are used for storing non-zero entries to save space.

Question 5. Explain About Transformations And Actions In The Context Of Rdds.
Answer :
Transformations are functions executed on demand, to produce a new RDD. All transformations are followed by actions. Some examples of transformations include map, filter and reduceByKey.
Actions are the results of RDD computations or transformations. After an action is performed, the data from RDD moves back to the local machine. Some examples of actions include reduce, collect, first, and take.

Question 7. Can You Use Spark To Access And Analyse Data Stored In Cassandra Databases?
Answer :
Yes, it is possible if you use Spark Cassandra Connector.

Question 8. Is It Possible To Run Apache Spark On Apache Mesos?
Answer :
Yes, Apache Spark can be run on the hardware clusters managed by Mesos.

Question 10. How Can Spark Be Connected To Apache Mesos?
Answer :
To connect Spark with Mesos-
Configure the spark driver program to connect to Mesos. Spark binary package should be in a location accessible by Mesos. (or)
Install Apache Spark in the same location as that of Apache Mesos and configure the property ‘spark.mesos.executor.home’ to point to the location where it is installed.

Question 11. How Can You Minimize Data Transfers When Working With Spark?
Answer :
Minimizing data transfers and avoiding shuffling helps write spark programs that run in a fast and reliable manner. The various ways in which data transfers can be minimized when working with Apache Spark are:
Using Broadcast Variable- Broadcast variable enhances the efficiency of joins between small and large RDDs.
Using Accumulators – Accumulators help update the values of variables in parallel while executing.
The most common way is to avoid operations ByKey, repartition or any other operations which trigger shuffles.

Question 12. Why Is There A Need For Broadcast Variables When Working With Apache Spark?
Answer :
These are read only variables, present in-memory cache on every machine. When working with Spark, usage of broadcast variables eliminates the necessity to ship copies of a variable for every task, so data can be processed faster. Broadcast variables help in storing a lookup table inside the memory which enhances the retrieval efficiency when compared to an RDD lookup ().

Question 13. Is It Possible To Run Spark And Mesos Along With Hadoop?
Answer :
Yes, it is possible to run Spark and Mesos with Hadoop by launching each of these as a separate service on the machines. Mesos acts as a unified scheduler that assigns tasks to either Spark or Hadoop.

Question 14. What Is Lineage Graph?
Answer :
The RDDs in Spark, depend on one or more other RDDs. The representation of dependencies in between RDDs is known as the lineage graph. Lineage graph information is used to compute each RDD on demand, so that whenever a part of persistent RDD is lost, the data that is lost can be recovered using the lineage graph information.

Question 15. How Can You Trigger Automatic Clean-ups In Spark To Handle Accumulated Metadata?
Answer :
You can trigger the clean-ups by setting the parameter ‘spark.cleaner.ttl’ or by dividing the long running jobs into different batches and writing the intermediary results to the disk.

Question 16. Explain About The Major Libraries That Constitute The Spark Ecosystem
Answer :
Spark MLib- Machine learning library in Spark for commonly used learning algorithms like clustering, regression, classification, etc.
Spark Streaming – This library is used to process real time streaming data.
Spark GraphX – Spark API for graph parallel computations with basic operators like joinVertices, subgraph, aggregateMessages, etc.
Spark SQL – Helps execute SQL like queries on Spark data using standard visualization or BI tools.
Question 17. What Are The Benefits Of Using Spark With Apache Mesos?
Answer :
It renders scalable partitioning among various Spark instances and dynamic partitioning between Spark and other big data frameworks.
Question 18. What Is The Significance Of Sliding Window Operation?
Answer :
Sliding Window controls transmission of data packets between various computer networks. Spark Streaming library provides windowed computations where the transformations on RDDs are applied over a sliding window of data. Whenever the window slides, the RDDs that fall within the particular window are combined and operated upon to produce new RDDs of the windowed DStream.
Question 19. What Is A Dstream?
Answer :
Discretized Stream is a sequence of Resilient Distributed Databases that represent a stream of data. DStreams can be created from various sources like Apache Kafka, HDFS, and Apache Flume. DStreams have two operations –
Transformations that produce a new DStream.
Output operations that write data to an external system.
Question 20. When Running Spark Applications, Is It Necessary To Install Spark On All The Nodes Of Yarn Cluster?
Answer :
Spark need not be installed when running a job under YARN or Mesos because Spark can execute on top of YARN or Mesos clusters without affecting any change to the cluster.
Question 21. What Is Catalyst Framework?
Answer :
Catalyst framework is a new optimization framework present in Spark SQL. It allows Spark to automatically transform SQL queries by adding new optimizations to build a faster processing system.
Question 22. Name A Few Companies That Use Apache Spark In Production.
Answer :
Pinterest, Conviva, Shopify, Open Table
Question 23. Which Spark Library Allows Reliable File Sharing At Memory Speed Across Different Cluster Frameworks?
Answer :
Tachyon
Work On Interesting Data Science Projects using Spark to build an impressive project portfolio!
Question 24. Why Is Blinkdb Used?
Answer :
BlinkDB is a query engine for executing interactive SQL queries on huge volumes of data and renders query results marked with meaningful error bars. BlinkDB helps users balance ‘query accuracy’ with response time.
Question 25. How Can You Compare Hadoop And Spark In Terms Of Ease Of Use?
Answer :
Hadoop MapReduce requires programming in Java which is difficult, though Pig and Hive make it considerably easier. Learning Pig and Hive syntax takes time. Spark has interactive APIs for different languages like Java, Python or Scala and also includes Shark i.e. Spark SQL for SQL lovers - making it comparatively easier to use than Hadoop.
Question 26. What Are The Common Mistakes Developers Make When Running Spark Applications?
Answer :
Developers often make the mistake of-
Hitting the web service several times by using multiple clusters.
Run everything on the local node instead of distributing it.
Developers need to be careful with this, as Spark makes use of memory for processing.
Question 27. What Is The Advantage Of A Parquet File?
Answer :
Parquet file is a columnar format file that helps –
Limit I/O operations
Consumes less space
Fetches only required columns.
Question 28. What Are The Various Data Sources Available In Sparksql?
Answer :
Parquet file
JSON Datasets
Hive tables
Question 29. How Spark Uses Hadoop?
Answer :
Spark has its own cluster management computation and mainly uses Hadoop for storage.
Question 30. What Are The Key Features Of Apache Spark That You Like?
Answer :
Spark provides advanced analytic options like graph algorithms, machine learning, streaming data, etc
It has built-in APIs in multiple languages like Java, Scala, Python and R
It has good performance gains, as it helps run an application in the Hadoop cluster ten times faster on disk and 100 times faster in memory.
Question 31. What Do You Understand By Pair Rdd?
Answer :
Special operations can be performed on RDDs in Spark using key/value pairs and such RDDs are referred to as Pair RDDs. Pair RDDs allow users to access each key in parallel. They have a reduceByKey () method that collects data based on each key and a join () method that combines different RDDs together, based on the elements having the same key.
Question 32. Which One Will You Choose For A Project –hadoop Mapreduce Or Apache Spark?
Answer :
As it is known that Spark makes use of memory instead of network and disk I/O. However, Spark uses large amount of RAM and requires dedicated machine to produce effective results. So the decision to use Hadoop or Spark varies dynamically with the requirements of the project and budget of the organization.
Question 33. Explain About The Different Types Of Transformations On Dstreams?
Answer :
Stateless Transformations- Processing of the batch does not depend on the output of the previous batch. Examples – map (), reduceByKey (), filter ().
Stateful Transformations- Processing of the batch depends on the intermediary results of the previous batch. Examples –Transformations that depend on sliding windows.
Question 34. Explain About The Popular Use Cases Of Apache Spark
Answer :
Apache Spark is mainly used for
Iterative machine learning.
Interactive data analytics and processing.
Stream processing
Sensor data processing
Question 35. Is Apache Spark A Good Fit For Reinforcement Learning?
Answer :
No. Apache Spark works well only for simple machine learning algorithms like clustering, regression, classification.
Question 36. What Is Spark Core?
Answer :
It has all the basic functionalities of Spark, like - memory management, fault recovery, interacting with storage systems, scheduling tasks, etc.
Question 37. How Can You Remove The Elements With A Key Present In Any Other Rdd?
Answer :
Use the subtractByKey () function
Question 38. What Is The Difference Between Persist() And Cache()
Answer :
persist () allows the user to specify the storage level whereas cache () uses the default storage level.
Question 39. What Are The Various Levels Of Persistence In Apache Spark?
Answer :
Apache Spark automatically persists the intermediary data from various shuffle operations, however it is often suggested that users call persist () method on the RDD in case they plan to reuse it. Spark has various persistence levels to store the RDDs on disk or in memory or as a combination of both with different replication levels.
The various storage/persistence levels in Spark are -
MEMORY_ONLY
MEMORY_ONLY_SER
MEMORY_AND_DISK
MEMORY_AND_DISK_SER, DISK_ONLY
OFF_HEAP
Question 40. How Spark Handles Monitoring And Logging In Standalone Mode?
Answer :
Spark has a web based user interface for monitoring the cluster in standalone mode that shows the cluster and job statistics. The log output for each job is written to the work directory of the slave nodes.
Question 41. Does Apache Spark Provide Check Pointing?
Answer :
Lineage graphs are always useful to recover RDDs from a failure but this is generally time consuming if the RDDs have long lineage chains. Spark has an API for check pointing i.e. a REPLICATE flag to persist. However, the decision on which data to checkpoint - is decided by the user. Checkpoints are useful when the lineage graphs are long and have wide dependencies.
Question 42. How Can You Launch Spark Jobs Inside Hadoop Mapreduce?
Answer :
Using SIMR (Spark in MapReduce) users can run any spark job inside MapReduce without requiring any admin rights.
Question 43. How Spark Uses Akka?
Answer :
Spark uses Akka basically for scheduling. All the workers request for a task to master after registering. The master just assigns the task. Here Spark uses Akka for messaging between the workers and masters.
Question 44. How Can You Achieve High Availability In Apache Spark?
Answer :
Implementing single node recovery with local file system
Using StandBy Masters with Apache ZooKeeper.
Question 45. Hadoop Uses Replication To Achieve Fault Tolerance. How Is This Achieved In Apache Spark?
Answer :
Data storage model in Apache Spark is based on RDDs. RDDs help achieve fault tolerance through lineage. RDD always has the information on how to build from other datasets. If any partition of a RDD is lost due to failure, lineage helps build only that particular lost partition.
Question 46. Explain About The Core Components Of A Distributed Spark Application.
Answer :
Driver- The process that runs the main () method of the program to create RDDs and perform transformations and actions on them.
Executor –The worker processes that run the individual tasks of a Spark job.
Cluster Manager-A pluggable component in Spark, to launch Executors and Drivers. The cluster manager allows Spark to run on top of other external managers like Apache Mesos or YARN.
Question 47. What Do You Understand By Lazy Evaluation?
Answer :
Spark is intellectual in the manner in which it operates on data. When you tell Spark to operate on a given dataset, it heeds the instructions and makes a note of it, so that it does not forget - but it does nothing, unless asked for the final result. When a transformation like map () is called on a RDD-the operation is not performed immediately. Transformations in Spark are not evaluated till you perform an action. This helps optimize the overall data processing workflow.
Question 48. Define A Worker Node.
Answer :
Worker node refers to any node that can run the application code in a cluster.
A node that can run the Spark application code in a cluster can be called as a worker node. A worker node can have more than one worker which is configured by setting the SPARK_ WORKER_INSTANCES property in the spark-env.sh file. Only one worker is started if the SPARK_ WORKER_INSTANCES property is not defined.
Question 49. What Do You Understand By Schemardd?
Answer :
An RDD that consists of row objects (wrappers around basic string or integer arrays) with schema information about the type of data in each column.
Question 50. What Are The Disadvantages Of Using Apache Spark Over Hadoop Mapreduce?
Answer :
Apache spark does not scale well for compute intensive jobs and consumes large number of system resources. Apache Spark’s in-memory capability at times comes a major roadblock for cost efficient processing of big data. Also, Spark does have its own file management system and hence needs to be integrated with other cloud based data platforms or apache hadoop.
Question 51. Is It Necessary To Install Spark On All The Nodes Of A Yarn Cluster While Running Apache Spark On Yarn ?
Answer :
No , it is not necessary because Apache Spark runs on top of YARN.
Question 52. What Do You Understand By Executor Memory In A Spark Application?
Answer :
When SparkContext connect to a cluster manager, it acquires an Executor on nodes in the cluster. Executors are Spark processes that run computations and store the data on the worker node. The final tasks by SparkContext are transferred to executors for their execution.
Every spark application has same fixed heap size and fixed number of cores for a spark executor. The heap size is what referred to as the Spark executor memory which is controlled with the spark.executor.memory property of the –executor-memory flag. Every spark application will have one executor on each worker node. The executor memory is basically a measure on how much memory of the worker node will the application utilize.
Question 53. What Does The Spark Engine Do?
Answer :
Spark engine schedules, distributes and monitors the data application across the spark cluster.
Question 54. What Makes Apache Spark Good At Low-latency Workloads Like Graph Processing And Machine Learning?
Answer :
Apache Spark stores data in-memory for faster model building and training. Machine learning algorithms require multiple iterations to generate a resulting optimal model and similarly graph algorithms traverse all the nodes and edges.These low latency workloads that need multiple iterations can lead to increased performance. Less disk access and  controlled network traffic make a huge difference when there is lots of data to be processed.
Question 55. Is It Necessary To Start Hadoop To Run Any Apache Spark Application ?
Answer :
Starting hadoop is not manadatory to run any spark application. As there is no seperate storage in Apache Spark, it uses Hadoop HDFS but it is not mandatory. The data can be stored in local file system, can be loaded from local file system and processed.
Question 56. What Is The Default Level Of Parallelism In Apache Spark?
Answer :
If the user does not explicitly specify then the number of partitions are considered as default level of parallelism in Apache Spark.
Question 57. Explain About The Common Workflow Of A Spark Program
Answer :
The foremost step in a Spark program involves creating input RDD's from external data.
Use various RDD transformations like filter() to create new transformed RDD's based on the business logic.
persist() any intermediate RDD's which might have to be reused in future.
Launch various RDD actions() like first(), count() to begin parallel computation , which will then be optimized and executed by Spark.
Question 58. Name A Few Commonly Used Spark Ecosystems.
Answer :
Spark SQL (Shark)
Spark Streaming
GraphX
MLlib
SparkR
Question 59. What Is “spark Sql”?
Answer :
Spark SQL is a Spark interface to work with structured as well as semi-structured data. It has the capability to load data from multiple structured sources like “text files”, JSON files, Parquet files, among others. Spark SQL provides a special type of RDD called SchemaRDD. These are row objects, where each object represents a record.
Question 60. Can We Do Real-time Processing Using Spark Sql?
Answer :
Not directly but we can register an existing RDD as a SQL table and trigger SQL queries on top of that.
Question 61. What Is Spark Sql?
Answer :
SQL Spark, better known as Shark is a novel module introduced in Spark to work with structured data and perform structured data processing. Through this module, Spark executes relational SQL queries on the data. The core of the component supports an altogether different RDD called SchemaRDD, composed of rows objects and schema objects defining data type of each column in the row. It is similar to a table in relational database.
Question 62. What Is A Parquet File?
Answer :
Parquet is a columnar format file supported by many other data processing systems. Spark SQL performs both read and write operations with Parquet file and consider it be one of the best big data analytics format so far.
Question 63. List The Functions Of Spark Sql.
Answer :
Spark SQL is capable of:
Loading data from a variety of structured sources
Querying data using SQL statements, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC). For instance, using business intelligence tools like Tableau
Providing rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more
Question 64. What Is Spark?
Answer :
Spark is a parallel data processing framework. It allows to develop fast, unified big data application combine batch, streaming and interactive analytics.
Question 65. What Is Hive On Spark?
Answer :
Hive is a component of Hortonworks’ Data Platform (HDP). Hive provides an SQL-like interface to data stored in the HDP. Spark users will automatically get the complete set of Hive’s rich features, including any new features that Hive might introduce in the future.
The main task around implementing the Spark execution engine for Hive lies in query planning, where Hive operator plans from the semantic analyzer which is translated to a task plan that Spark can execute. It also includes query execution, where the generated Spark plan gets actually executed in the Spark cluster.
Question 66. What Is A “parquet” In Spark?
Answer :
“Parquet” is a columnar format file supported by many data processing systems. Spark SQL performs both read and write operations with the “Parquet” file.
Question 67. What Are Benefits Of Spark Over Mapreduce?
Answer :
Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than Hadoop MapReduce. MapReduce makes use of persistence storage for any of the data processing tasks.
Unlike Hadoop, Spark provides in-built libraries to perform multiple tasks form the same core like batch processing, Steaming, Machine learning, Interactive SQL queries. However, Hadoop only supports batch processing.
Hadoop is highly disk-dependent whereas Spark promotes caching and in-memory data storage
Spark is capable of performing computations multiple times on the same dataset. This is called iterative computation while there is no iterative computing implemented by Hadoop.
Question 68. How Sparksql Is Different From Hql And Sql?
Answer :
SparkSQL is a special component on the spark Core engine that support SQL and Hive Query Language without changing any syntax. It’s possible to join SQL table and HQL table.
=====================

===========================
 
Q: Can you explain the key features of Apache Spark?
A:

Support for Several Programming Languages – Spark code can be written in any of the four programming languages, namely Java, Python, R, and Scala. It also provides high-level APIs in these programming languages. Additionally, Apache Spark provides shells in Python and Scala. The Python shell is accessed through the ./bin/pyspark directory, while for accessing the Scala shell one needs to go to the .bin/spark-shell directory.
Lazy Evaluation – Apache Spark makes use of the concept of lazy evaluation, which is to delay the evaluation up until the point it becomes absolutely compulsory.
Machine Learning – For big data processing, Apache Spark’s MLib machine learning component is useful. It eliminates the need for using separate engines for processing and machine learning.
Multiple Format Support – Apache Spark provides support for multiple data sources, including Cassandra, Hive, JSON, and Parquet. The Data Sources API offers a pluggable mechanism for accessing structured data via Spark SQL. These data sources can be much more than just simple pipes able to convert data and pulling the same into Spark.
Real-Time Computation – Spark is designed especially for meeting massive scalability requirements. Thanks to its in-memory computation, Spark’s computation is real-time and has less latency.
Speed – For large-scale data processing, Spark can be up to 100 times faster than Hadoop MapReduce. Apache Spark is able to achieve this tremendous speed via controlled portioning. The distributed, general-purpose cluster-computing framework manages data by means of partitions that help in parallelizing distributed data processing with minimal network traffic.
Hadoop Integration – Spark offers smooth connectivity with Hadoop. In addition to being a potential replacement for the Hadoop MapReduce functions, Spark is able to run on top of an extant Hadoop cluster by means of YARN for resource scheduling.
Q: What advantages does Spark offer over Hadoop MapReduce?
A:

Enhanced Speed – MapReduce makes use of persistent storage for carrying out any of the data processing tasks. On the contrary, Spark uses in-memory processing that offers about 10 to 100 times faster processing than the Hadoop MapReduce.
Multitasking – Hadoop only supports batch processing via inbuilt libraries. Apache Spark, on the other end, comes with built-in libraries for performing multiple tasks from the same core, including batch processing, interactive SQL queries, machine learning, and streaming.
No Disk-Dependency – While Hadoop MapReduce is highly disk-dependent, Spark mostly uses caching and in-memory data storage.
Iterative Computation – Performing computations several times on the same dataset is termed as iterative computation. Spark is capable of iterative computation while Hadoop MapReduce isn’t.
Q: Please explain the concept of RDD (Resilient Distributed Dataset). Also, state how you can create RDDs in Apache Spark.
A: An RDD or Resilient Distribution Dataset is a fault-tolerant collection of operational elements that are capable to run in parallel. Any partitioned data in an RDD is distributed and immutable.

Fundamentally, RDDs are portions of data that are stored in the memory distributed over many nodes. These RDDs are lazily evaluated in Spark, which is the main factor contributing to the hastier speed achieved by Apache Spark. RDDs are of two types:

Hadoop Datasets – Perform functions on each file record in HDFS (Hadoop Distributed File System) or other types of storage systems
Parallelized Collections – Extant RDDs running parallel with one another
There are two ways of creating an RDD in Apache Spark:

By parallelizing a collection in the Driver program. It makes use of SparkContext’s parallelize() method. For instance:
method val DataArray = Array(22,24,46,81,101) val DataRDD = sc.parallelize(DataArray)
By means of loading an external dataset from some external storage, including HBase, HDFS, and shared file system
Q: What are the various functions of Spark Core?
A: Spark Core acts as the base engine for large-scale parallel and distributed data processing. It is the distributed execution engine used in conjunction with the Java, Python, and Scala APIs that offer a platform for distributed ETL (Extract, Transform, Load) application development.


 
Various functions of Spark Core are:

Distributing, monitoring, and scheduling jobs on a cluster
Interacting with storage systems
Memory management and fault recovery
Furthermore, additional libraries built on top of the Spark Core allow it to diverse workloads for machine learning, streaming, and SQL query processing.

Q: Please enumerate the various components of the Spark Ecosystem.
A:

GraphX – Implements graphs and graph-parallel computation
MLib – Used for machine learning
Spark Core – Base engine used for large-scale parallel and distributed data processing
Spark Streaming – Responsible for processing real-time streaming data
Spark SQL – Integrates Spark’s functional programming API with relational processing
Q: Is there any API available for implementing graphs in Spark?
A: GraphX is the API used for implementing graphs and graph-parallel computing in Apache Spark. It extends the Spark RDD with a Resilient Distributed Property Graph. It is a directed multi-graph that can have several edges in parallel.

Each edge and vertex of the Resilient Distributed Property Graph has user-defined properties associated with it. The parallel edges allow for multiple relationships between the same vertices.

In order to support graph computation, GraphX exposes a set of fundamental operators, such as joinVertices, mapReduceTriplets, and subgraph, and an optimized variant of the Pregel API.

The GraphX component also includes an increasing collection of graph algorithms and builders for simplifying graph analytics tasks.

Q: Tell us how will you implement SQL in Spark?
A: Spark SQL modules help in integrating relational processing with Spark’s functional programming API. It supports querying data via SQL or HiveQL (Hive Query Language).


 
Also, Spark SQL supports a galore of data sources and allows for weaving SQL queries with code transformations. DataFrame API, Data Source API, Interpreter & Optimizer, and SQL Service are the four libraries contained by the Spark SQL.

Q: What do you understand by the Parquet file?
A: Parquet is a columnar format that is supported by several data processing systems. With it, Spark SQL performs both read as well as write operations. Having columnar storage has the following advantages:

Able to fetch specific columns for access
Consumes less space
Follows type-specific encoding
Limited I/O operations
Offers better-summarized data
Q: Can you explain how you can use Apache Spark along with Hadoop?
A: Having compatibility with Hadoop is one of the leading advantages of Apache Spark. The duo makes up for a powerful tech pair. Using Apache Spark and Hadoop allows for making use of Spark’s unparalleled processing power in line with the best of Hadoop’s HDFS and YARN abilities.

Following are the ways of using Hadoop Components with Apache Spark:

Batch & Real-Time Processing – MapReduce and Spark can be used together where the former handles the batch processing and the latter is responsible for real-time processing
HDFS – Spark is able to run on top of the HDFS for leveraging the distributed replicated storage
MapReduce – It is possible to use Apache Spark along with MapReduce in the same Hadoop cluster or independently as a processing framework
YARN – Spark applications can run on YARN
Q: Name various types of Cluster Managers in Spark.
A:

Apache Mesos – Commonly used cluster manager
Standalone – A basic cluster manager for setting up a cluster
YARN – Used for resource management
Q: Is it possible to use Apache Spark for accessing and analyzing data stored in Cassandra databases?
A: Yes, it is possible to use Apache Spark for accessing as well as analyzing data stored in Cassandra databases using the Spark Cassandra Connector. It needs to be added to the Spark project during which a Spark executor talks to a local Cassandra node and will query only local data.

Connecting Cassandra with Apache Spark allows making queries faster by means of reducing the usage of network for sending data between Spark executors and Cassandra nodes.

Q: Please explain the sparse vector in Spark.
A: A sparse vector is used for storing non-zero entries for saving space. It has two parallel arrays:

One for indices
The other for values
An example of a sparse vector is as follows:

Vectors.sparse(7,Array(0,1,2,3,4,5,6),Array(1650d,50000d,800d,3.0,3.0,2009,95054))

Q: How will you connect Apache Spark with Apache Mesos?
A: Step by step procedure for connecting Apache Spark with Apache Mesos is:

Configure the Spark driver program to connect with Apache Mesos
Put the Spark binary package in a location accessible by Mesos
Install Apache Spark in the same location as that of the Apache Mesos
Configure the spark.mesos.executor.home property for pointing to the location where the Apache Spark is installed
Q: Can you explain how to minimize data transfers while working with Spark?
A: Minimizing data transfers as well as avoiding shuffling helps in writing Spark programs capable of running reliably and fast. Several ways for minimizing data transfers while working with Apache Spark are:

Avoiding – ByKey operations, repartition, and other operations responsible for triggering shuffles
Using Accumulators – Accumulators provide a way for updating the values of variables while executing the same in parallel
Using Broadcast Variables – A broadcast variable helps in enhancing the efficiency of joins between small and large RDDs
Q: What are broadcast variables in Apache Spark? Why do we need them?
A: Rather than shipping a copy of a variable with tasks, a broadcast variable helps in keeping a read-only cached version of the variable on each machine.

Broadcast variables are also used to provide every node with a copy of a large input dataset. Apache Spark tries to distribute broadcast variable by using effectual broadcast algorithms for reducing communication cost.

Using broadcast variables eradicates the need of shipping copies of a variable for each task. Hence, data can be processed quickly. Compared to an RDD lookup(), broadcast variables assist in storing a lookup table inside the memory that enhances the retrieval efficiency.

Q: Please provide an explanation on DStream in Spark.
A: DStream is a contraction for Discretized Stream. It is the basic abstraction offered by Spark Streaming and is a continuous stream of data. DStream is received from either a processed data stream generated by transforming the input stream or directly from a data source.

A DStream is represented by a continuous series of RDDs, where each RDD contains data from some certain interval. An operation applied to a DStream is analogous to applying the same operation on the underlying RDDs. A DStream has two operations:

Output operations responsible for writing data to an external system
Transformations resulting in the production of a new DStream
It is possible to create DStream from various sources, including Apache Kafka, Apache Flume, and HDFS. Also, Spark Streaming provides support for several DStream transformations.

Q: Does Apache Spark provide checkpoints?
A: Yes, Apache Spark provides checkpoints. They allow for a program to run all around the clock in addition to making it resilient towards failures not related to application logic. Lineage graphs are used for recovering RDDs from a failure.

Apache Spark comes with an API for adding and managing checkpoints. The user then decides which data to the checkpoint. Checkpoints are preferred over lineage graphs when the latter are long and have wider dependencies.

Q: What are the different levels of persistence in Spark?
A: Although the intermediary data from different shuffle operations automatically persists in Spark, it is recommended to use the persist () method on the RDD if the data is to be reused.

Apache Spark features several persistence levels for storing the RDDs on disk, memory, or a combination of the two with distinct replication levels. These various persistence levels are:

DISK_ONLY – Stores the RDD partitions only on the disk.
MEMORY_AND_DISK – Stores RDD as deserialized Java objects in the JVM. In case the RDD isn’t able to fit in the memory, additional partitions are stored on the disk. These are read from here each time the requirement arises.
MEMORY_ONLY_SER – Stores RDD as serialized Java objects with one byte array per partition.
MEMORY_AND_DISK_SER – Identical to MEMORY_ONLY_SER with the exception of storing partitions not able to fit in the memory to the disk in place of recomputing them on the fly when required.
MEMORY_ONLY – The default level, it stores the RDD as deserialized Java objects in the JVM. In case the RDD isn’t able to fit in the memory available, some partitions won’t be cached, resulting into recomputing the same on the fly every time they are required.
OFF_HEAP – Works like MEMORY_ONLY_SER but stores the data in off-heap memory.
Q: Can you list down the limitations of using Apache Spark?
A:

Doesn’t have a built-in file management system. Hence, it needs to be integrated with other platforms like Hadoop for benefitting from a file management system
Higher latency but consequently, lower throughput
No support for true real-time data stream processing. The live data stream is partitioned into batches in Apache Spark and after processing are again converted into batches. Hence, Spark Streaming is a micro-batch processing and not truly real-time data processing
Lesser number of algorithms available
Spark streaming doesn’t support record-based window criteria
The work needs to be distributed over multiple clusters instead of running everything on a single node
While using Apache Spark for cost-efficient processing of big data, its ‘in-memory’ ability becomes a bottleneck
That completes the list of the 20 important Spark interview questions. Going through these questions will allow you to check your Spark knowledge as well as help prepare for an upcoming Apache Spark interview. All the best.
=============================

How to run spark in Standalone client mode?

spark-submit \
class org.apache.spark.examples.SparkPi \
deploy-mode client \
master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
How to run spark in Standalone cluster mode?

spark-submit \
class org.apache.spark.examples.SparkPi \
deploy-mode cluster \
master spark//$SPARK_MASTER_IP:$SPARK_MASTER_PORT \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
How to run spark in YARN client mode?

spark-submit \
class org.apache.spark.examples.SparkPi \
deploy-mode client \
master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
How to run spark in YARN cluster mode?

spark-submit \
class org.apache.spark.examples.SparkPi \
deploy-mode cluster \
master yarn \
$SPARK_HOME/examples/lib/spark-examples_version.jar 10
What is Executor memory?

You can configure this using the –executor-memory argument to sparksubmit. Each application will have at most one executor on each worker, so this setting controls how much of that worker’s memory the application will claim. By default, this setting is 1 GB—you will likely want to increase it on most servers.

What is the maximum number of total cores?

This is the total number of cores used across all executors for an application. By default, this is unlimited; that is, the application will launch executors on every available node in the cluster. For a multiuser workload, you should instead ask users to cap their usage. You can set this value through the –total-execution cores argument to spark-submit, or by configuring spark.cores.max in your Spark configuration file.


Define Partitions?

As the name suggests, partition is a smaller and logical division of data similar to ‘split’ in MapReduce. Partitioning is the process to derive logical units of data to speed up the processing process. Everything in Spark is a partitioned RDD.

What operations RDD support?

Transformations
Actions
What do you understand by Transformations in Spark?

Transformations are functions applied on RDD, resulting into another RDD. It does not execute until an action occurs. map() and filer() are examples of transformations, where the former applies the function passed to it on each element of RDD and results into another RDD. The filter() creates a new RDD by selecting elements form current RDD that pass function argument.

Define Actions.

An action helps in bringing back the data from RDD to the local machine. An action’s execution is the result of all previously created transformations. reduce() is an action that implements the function passed again and again until one value if left. take() action takes all the values from RDD to local node.

What is RDD Lineage?

Spark does not support data replication in the memory and thus, if any data is lost, it is rebuild using RDD lineage. RDD lineage is a process that reconstructs lost data partitions. The best is that RDD always remembers how to build from other datasets.

What is Spark Driver?

Spark Driver is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. In simple terms, driver in Spark creates SparkContext, connected to a given Spark Master. The driver also delivers the RDD graphs to Master, where the standalone cluster manager runs.

What is Hive on Spark?

Hive contains significant support for Apache Spark, wherein Hive execution is configured to Spark:

hive> set spark.home=/location/to/sparkHome;
hive> set hive.execution.engine=spark;
Hive on Spark supports Spark on yarn mode by default.
Name commonly-used Spark Ecosystems?

Spark SQL (Shark)- for developers
Spark Streaming for processing live data streams
GraphX for generating and computing graphs
MLlib (Machine Learning Algorithms)
SparkR to promote R Programming in Spark engine.
What are the main components of Spark?

Spark Core: Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines RDDs,
Spark SQL: Spark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the HQL.
Spark Streaming: Spark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers.
MLlib: Spark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms.
GraphX: GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations.
How Spark Streaming works?

Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs.

Define Spark Streaming.Spark supports stream processing?

An extension to the Spark API , allowing stream processing of live data streams. The data from different sources like Flume, HDFS is streamed and finally processed to file systems, live dashboards and databases. It is similar tobatch processing as the input data is divided into streams like batches.

What is GraphX?

Spark uses GraphX for graph processing to build and transform interactive graphs. The GraphX component enables programmers to reason about structured data at scale.

What does MLlib do?

MLlib is scalable machine learning library provided by Spark. It aims at making machine learning easy and scalable with common learning algorithms and use cases like clustering, regression filtering, dimensional reduction, and alike.

What is Spark SQL?

SQL Spark, better known as Shark is a novel module introduced in Spark to work with structured data and perform structured data processing. Through this module, Spark executes relational SQL queries on the data. The core of the component supports an altogether different RDD called SchemaRDD, composed of rows objects and schema objects defining data type of each column in the row. It is similar to a table in relational database.

What file systems Spark support?

Hadoop Distributed File System (HDFS)
Local File system
S320.What is Yarn?Similar to Hadoop, Yarn is one of the key features in Spark, providing a central and resource management platform to deliver scalable operations across the cluster . Running Spark on Yarn necessitates a binary distribution of Spar as built on Yarn support.
List the functions of Spark SQL?

Spark SQL is capable of:

Loading data from a variety of structured sources
Querying data using SQL statements, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC). For instance, using business intelligence tools like Tableau
Providing rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more
What is persist()?

Spark’s RDDs are by default recomputed each time you run an action on them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist().After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions. Persisting RDDs on disk instead of memory is also possible.

Write common workflow of a Spark program?

Every Spark program and shell session will work as follows:

Create some input RDDs from external data.
Transform them to define new RDDs using transformations like filter().
Ask Spark to persist() any intermediate RDDs that will need to be reused.
Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and  executed by Spark.
Difference between cache() and persist()?

With cache(), you use only the default storage level MEMORY_ONLY. With persist(), you can specify which storage level you want.So cache() is the same as calling persist() with the default storage level.Spark has many levels of persistence to choose from based on what our goals are.The default persist() will store the data in the JVM heap as unserialized objects. When we write data out to disk, that data is also always serialized.Different levels of persistence are MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK, MEMORY_AND_DISK_SER, DISK_ONLY.

What is lineage graph?

As you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. It uses this information to compute each RDD on demand and to recover lost data if part of a persistent RDD is lost.

Difference between map() and flatMap()?

The map() transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. Sometimes we want to produce multiple output elements for each input element. The operation to do this is called flatMap(). As with map(), the function we provide to flatMap() is called individually for each element in our input RDD. Instead of returning a single element, we return an iterator with our return values.

What is reduce() action?

It takes a function that operates on two elements of the type in your RDD and returns a new element of the same type. A simple example of such a function is +, which we can use to sum our RDD. With reduce(), we can easily sum the elements of our RDD, count the number of elements, and perform other types of aggregations.

What is Pair RDD?

Spark provides special operations on RDDs containing key/value pairs. These RDDs are called pair RDDs. Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel.For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key.

What are Accumulators?

Accumulators, provides a simple syntax for aggregating values from worker nodes back to the driver program. One of the most common uses of accumulators is to count events that occur during job execution for debugging purposes.

What is Broadcast Variables?

Spark’s second type of shared variable, broadcast variables, allows the program to efficiently send a large, read-only value to all the worker nodes for use in one or more Spark operations. They come in handy, for example, if your application needs to send a large, read-only lookup table to all the nodes.

What is Piping?

Spark provides a pipe() method on RDDs. Spark’s pipe() lets us write parts of jobs using any language we want as long as it can read and write to Unix standard streams. With pipe(), you can write a transformation of an RDD that reads each RDD element from standard input as a String, manipulates that String however you like, and then writes the result(s) as Strings to standard output.

What are benefits of Spark over MapReduce?

Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than  Hadoop MapReduce. MapReduce makes use of persistence storage for any of the data processing tasks.
Unlike Hadoop, Spark provides in-built libraries to perform multiple tasks form the same core like batch  processing, Steaming, Machine learning, Interactive SQL queries. However, Hadoop only supports batch     processing.
Hadoop is highly disk-dependent whereas Spark promotes caching and in-memory data storage
Spark is capable of performing computations multiple times on the same dataset. This is called iterative  computation while there is no iterative computing implemented by Hadoop.
Is there any benefit of learning MapReduce, then?

Yes, MapReduce is a paradigm used by many big data tools including Spark as well. It is extremely relevant to use MapReduce when the data grows bigger and bigger. Most tools like Pig and Hive convert their queries into MapReduce phases to optimize them better.


What is a schema RDD/DataFrame?

A SchemaRDD is an RDD composed of Row objects with additional schema information of the types in each column. Row objects are just wrappers around arrays of basic types (e.g., integers and strings).

What are Row objects?

Row objects represent records inside SchemaRDDs, and are simply fixed-length arrays of fields.Row objects have a number of getter functions to obtain the value of each field given its index. The standard getter, get (or apply in Scala), takes a column number and returns an Object type (or Any in Scala) that we are responsible for casting to the correct type. For Boolean, Byte, Double, Float, Int, Long, Short, and String, there is a getType() method, which returns that type. For example, get String(0) would return field 0 as a string.

Explain Spark Streaming Architecture?

Spark Streaming uses a “micro-batch” architecture, where Spark Streaming receives data from various input sources and groups it into small batches. New batches are created at regular time intervals. At the beginning of each time interval a new batch is created, and any data that arrives during that interval gets added to that batch. At the end of the time interval the batch is done growing. The size of the time intervals is determined by a parameter called the batch interval. Each input batch forms an RDD,  and is processed using Spark jobs to create other RDDs. The processed results can then be pushed out to external systems in batches.


How Spark achieves fault tolerance?

Spark stores data in-memory whereas Hadoop stores data on disk. Hadoop uses replication to achieve fault tolerance whereas Spark uses different data storage model, RDD. RDDs achieve fault tolerance through a notion of lineage: if a partition of an RDD is lost, the RDD has enough information to rebuild just that partition.This removes the need for replication to achieve fault tolerance.

What are Spark’s main features?

Speed : Spark enables applications in Hadoop clusters to run up to 100x faster in memory, and 10x faster even when running on disk. Spark makes it possible by reducing number of read/write to disc. It stores this intermediate processing data in-memory. It uses the concept of an Resilient Distributed Dataset (RDD), which allows it to transparently store data on memory and persist it to disc only it’s needed. This helps to reduce most of the disc read and write – the main time consuming factors – of data processing.
Combines SQL, streaming, and complex analytics: In addition to simple “map” and “reduce” operations, Spark supports SQL queries, streaming data, and complex analytics such as machine learning and graph algorithms out-of-the-box. Not only that, users can combine all these capabilities seamlessly in a single workflow.
Ease of Use:Spark lets you quickly write applications in Java, Scala, or Python. This helps developers to create and run their applications on their familiar programming languages and easy to build parallel apps.
Runs Everywhere: Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, S3.
Explain about the popular use cases of Apache Spark?

Apache Spark is mainly used for
Iterative machine learning.
Interactive data analytics and processing.
Stream processing
Sensor data processing
Is Apache Spark a good fit for Reinforcement learning?

No. Apache Spark works well only for simple machine learning algorithms like clustering, regression, classification.

What is Spark Core?

It has all the basic functionalities of Spark, like - memory management, fault recovery, interacting with storage systems, scheduling tasks, etc.

How can you remove the elements with a key present in any other RDD?

Use the subtractByKey () function

What is the difference between persist() and cache()

persist () allows the user to specify the storage level whereas cache () uses the default storage level.

What are the various levels of persistence in Apache Spark?

Apache Spark automatically persists the intermediary data from various shuffle operations, however it is often suggested that users call persist () method on the RDD in case they plan to reuse it. Spark has various persistence levels to store the RDDs on disk or in memory or as a combination of both with different replication levels.

The various storage/persistence levels in Spark are -

MEMORY_ONLY
MEMORY_ONLY_SER
MEMORY_AND_DISK
MEMORY_AND_DISK_SER, DISK_ONLY
OFF_HEAP
How Spark handles monitoring and logging in Standalone mode?

Spark has a web based user interface for monitoring the cluster in standalone mode that shows the cluster and job statistics. The log output for each job is written to the work directory of the slave nodes.

Does Apache Spark provide checkpointing?

Lineage graphs are always useful to recover RDDs from a failure but this is generally time consuming if the RDDs have long lineage chains. Spark has an API for check pointing i.e. a REPLICATE flag to persist. However, the decision on which data to checkpoint - is decided by the user. Checkpoints are useful when the lineage graphs are long and have wide dependencies.

How can you launch Spark jobs inside Hadoop MapReduce?

Using SIMR (Spark in MapReduce) users can run any spark job inside MapReduce without requiring any admin rights.

Spark Tutorials

How Spark uses Akka?

Spark uses Akka basically for scheduling. All the workers request for a task to master after registering. The master just assigns the task. Here Spark uses Akka for messaging between the workers and masters.

How can you achieve high availability in Apache Spark?

Implementing single node recovery with local file system Using StandBy Masters with Apache ZooKeeper.

Hadoop uses replication to achieve fault tolerance. How is this achieved in Apache Spark?

Data storage model in Apache Spark is based on RDDs. RDDs help achieve fault tolerance through lineage. RDD always has the information on how to build from other datasets. If any partition of a RDD is lost due to failure, lineage helps build only that particular lost partition.

Explain about the core components of a distributed Spark application.

Driver- The process that runs the main () method of the program to create RDDs and perform transformations and actions on them.
Executor –The worker processes that run the individual tasks of a Spark job.
Cluster Manager-A pluggable component in Spark, to launch Executors and Drivers. The cluster manager allows Spark to run on top of other external managers like Apache Mesos or YARN.

What do you understand by Lazy Evaluation?

Spark is intellectual in the manner in which it operates on data. When you tell Spark to operate on a given dataset, it heeds the instructions and makes a note of it, so that it does not forget - but it does nothing, unless asked for the final result. When a transformation like map () is called on a RDD-the operation is not performed immediately. Transformations in Spark are not evaluated till you perform an action. This helps optimize the overall data processing workflow.

Define a worker node?

A node that can run the Spark application code in a cluster can be called as a worker node. A worker node can have more than one worker which is configured by setting the SPARK_ WORKER_INSTANCES property in the spark-env.sh file. Only one worker is started if the SPARK_ WORKER_INSTANCES property is not defined.

What do you understand by SchemaRDD?

An RDD that consists of row objects (wrappers around basic string or integer arrays) with schema information about the type of data in each column.

1. Compare MapReduce and Spark?
Criteria				MapReduce				Spark
Processing Speeds			Good					Excellent (up to 100 times faster)
Data caching				Hard disk				In-memory
Perform iterative jobs			Average					Excellent
Independent of Hadoop			No					Yes
Machine learning applications		Average					Excellent

2. What is Apache Spark?
Spark is a fast, easy-to-use and flexible data processing framework. It has an advanced execution engine supporting cyclic data  flow and in-memory computing. Spark can run on Hadoop, standalone or in the cloud and is capable of accessing diverse data sources including HDFS, HBase, Cassandra and others.

3. Explain key features of Spark.
Allows Integration with Hadoop and files included in HDFS.
Spark has an interactive language shell as it has an independent Scala (the language in which Spark is written) interpreter.
Spark consists of RDD’s (Resilient Distributed Datasets), which can be cached across computing nodes in a cluster.
Spark supports multiple analytic tools that are used for interactive query analysis , real-time analysis and graph processing

5. What does a Spark Engine do?
Spark Engine is responsible for scheduling, distributing and monitoring the data application across the cluster.

Find out more about what the Spark Engine does in this Apache Spark Video.

6. Define Partitions?
As the name suggests, partition is a smaller and logical division of data  similar to ‘split’ in MapReduce. Partitioning is the process to derive logical units of data to speed up the processing process. Everything in Spark is a partitioned RDD.

7. What operations RDD support?
Transformations.
Actions

8. What do you understand by Transformations in Spark?
Transformations are functions applied on RDD, resulting into another RDD. It does not execute until an action occurs. map() and filer() are examples of transformations, where the former applies the function passed to it on each element of RDD and results into another RDD. The filter() creates a new RDD by selecting elements form current RDD that pass function argument.

9. Define Actions.
An action helps in bringing back the data from RDD to the local machine. An action’s execution is the result of all previously created transformations. reduce() is an action that implements the function passed again and again until one value if left. take() action takes all the values from RDD to local node.

Learn Spark from Experts! Enrol Today

10. Define functions of SparkCore?
Serving as the base engine, SparkCore performs various important functions like memory management, monitoring jobs, fault-tolerance, job scheduling and interaction with storage systems.

11. What is RDD Lineage?
Spark does not support data replication in the memory and thus, if any data is lost, it is rebuild using RDD lineage. RDD lineage is a process that reconstructs lost data partitions. The best is that RDD always remembers how to build from other datasets.

12. What is Spark Driver?
Spark Driver is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. In simple terms, driver in Spark creates SparkContext, connected to a given Spark Master.
The driver also delivers the RDD graphs to Master, where the standalone cluster manager runs.

Are you interested in the comprehensive Apache Spark and Scala Videos to take your career to the next level?

13. What is Hive on Spark?
Hive contains significant support for Apache Spark, wherein Hive execution is configured to Spark:

hive> set spark.home=/location/to/sparkHome;
hive> set hive.execution.engine=spark;
Hive on Spark supports Spark on yarn mode by default.

14. Name commonly-used Spark Ecosystems.
Spark SQL (Shark)- for developers.
Spark Streaming for processing live data streams.
GraphX for generating and computing graphs.
MLlib (Machine Learning Algorithms).
SparkR to promote R Programming in Spark engine.

15. Define Spark Streaming.
Spark supports stream processing – an extension to the Spark API , allowing stream processing of live data streams. The data from different sources like Flume, HDFS is streamed and finally processed to file systems, live dashboards and databases. It is similar to batch processing as the input data is divided into streams like batches.

16. What is GraphX?
Spark uses GraphX for graph processing to build and transform interactive graphs. The GraphX component enables programmers to reason about structured data at scale.

17. What does MLlib do?
MLlib is scalable machine learning library provided by Spark. It aims at making machine learning easy and scalable with common learning algorithms and use cases like clustering, regression filtering, dimensional reduction, and alike.

Our in-depth Scala Certification Course can give your career a big boost!

18. What is Spark SQL?
SQL Spark, better known as Shark is a novel module introduced in Spark to work with structured data and perform structured data processing. Through this module, Spark executes relational SQL queries on the data. The core of the component supports an altogether different RDD called SchemaRDD, composed of rows objects and schema objects defining data type of each column in the row. It is similar to a table in relational database.

20. What file systems Spark support?
Hadoop Distributed File System (HDFS).
Local File system.
S3

22. List the functions of Spark SQL.?
Spark SQL is capable of:

Loading data from a variety of structured sources.
Querying data using SQL statements, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC). For instance, using business intelligence tools like Tableau.
Providing rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more.

23. What are benefits of Spark over MapReduce?
Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than Hadoop MapReduce. MapReduce makes use of persistence storage for any of the data processing tasks.
Unlike Hadoop, Spark provides in-built libraries to perform multiple tasks form the same core like batch processing, Steaming, Machine learning, Interactive SQL queries. However, Hadoop only supports batch processing.
Hadoop is highly disk-dependent whereas Spark promotes caching and in-memory data storage.
Spark is capable of performing computations multiple times on the same dataset. This is called iterative computation while there is no iterative computing implemented by Hadoop.
Read more in this blog about the comparison of Spark and MapReduce.

24. Is there any benefit of learning MapReduce, then?
Yes, MapReduce is a paradigm used by many big data tools including Spark as well. It is extremely relevant to use MapReduce when the data grows bigger and bigger. Most tools like Pig and Hive convert their queries into MapReduce phases to optimize them better.


26. Name types of Cluster Managers in Spark.
The Spark framework supports three major types of Cluster Managers:

Standalone : a basic manager to set up a cluster.
Apache Mesos : generalized/commonly-used cluster manager, also runs Hadoop MapReduce and other applications.
Yarn : responsible for resource management in Hadoop

28. What is PageRank?
A unique feature and algorithm in graph, PageRank is the measure of each vertex in the graph. For instance, an edge from u to v represents endorsement of v’s importance by u. In simple terms, if a user at Instagram is followed massively, it will rank high on that platform.

29. Do you need to install Spark on all nodes of Yarn cluster while running Spark on Yarn?
No because Spark runs on top of Yarn.

30. Illustrate some demerits of using Spark.
Since Spark utilizes more storage space compared to Hadoop and MapReduce, there may arise certain problems. Developers need to be careful while running their applications in Spark. Instead of running everything on a single node, the work must be distributed over multiple clusters.

31. How to create RDD?
Spark provides two methods to create RDD:• By parallelizing a collection in your Driver program. This makes use of SparkContext’s ‘parallelize’ methodval IntellipaatData = Array(2,4,6,8,10)
val distIntellipaatData = sc.parallelize(IntellipaatData)• By loading an external dataset from external storage like HDFS, shared file system.


