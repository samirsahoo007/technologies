df - A pandas DataFrame object s - A pandas Series object (https://s3.amazonaws.com/dq-blog-files/pandas-cheat-sheet.pdf)

IMPORTS
=======
import pandas as pd
import numpy as np

IMPORTING DATA
==============
pd.read_csv(filename) 			- From a CSV file 
pd.read_table(filename) 		- From a delimited text file (like TSV)
pd.read_excel(filename) 		- From an Excel file
pd.read_sql(query, connection_object) 	- Reads from a SQL table/database 
pd.read_json(json_string) 		- Reads from a JSON formatted string, URL or file. 
pd.read_html(url) 			- Parses an html URL, string or file and extracts tables to a list of dataframes 
pd.read_clipboard() 			- Takes the contents of your clipboard and passes it to read_table() 
pd.DataFrame(dict) 			- From a dict, keys for columns names, values for data as lists

EXPORTING DATA
==============
df.to_csv(filename) 				- Writes to a CSV file 
df.to_excel(filename) 				- Writes to an Excel file 
df.to_sql(table_name, connection_object) 	- Writes to a SQL table 
df.to_json(filename) 				- Writes to a file in JSON format
df.to_html(filename) 				- Saves as an HTML table 
df.to_clipboard() 				- Writes to the clipboard

Read and Write to SQL Query or Database Table
=============================================
read_sql()is a convenience wrapper around read_sql_table() and read_sql_query()
 >>> pd.read_csv(' le.csv', header=None, nrows=5) >>> df.to_csv('myDataFrame.csv')
>>> pd.read_excel(' le.xlsx')
>>> pd.to_excel('dir/myDataFrame.xlsx', sheet_name='Sheet1')

Read multiple sheets from the same  le
>>> xlsx = pd.ExcelFile(' le.xls')
>>> df = pd.read_excel(xlsx, 'Sheet1')
>>> from sqlalchemy import create_engine			- SQLAlchemy is an open-source SQL toolkit and object-relational mapper (ORM) for the Python 
>>> engine = create_engine('sqlite:///:memory:'			- SQLAlchemy is a library that facilitates the communication between Python programs and databases. 
								- Most of the times, this library is used as an Object Relational Mapper (ORM) tool that translates 
								- Python classes to tables on relational databases and automatically converts function calls to SQL statements.)
								- It's mostly used to define a relationship between an object/class and the database table where it's data should be saved.
								Does Django use SQLAlchemy?
								In some cases, Django and SQLAlchemy can be used together. The main use case I got to see numerous times in the real world is 
								when Django is used for all regular CRUD operations, while SQLAlchemy is used for the more complex queries, usually read-only queries

>>> pd.read_sql("SELECT * FROM my_table;", engine)
>>> pd.read_sql_table('my_table', engine)
>>> pd.read_sql_query("SELECT * FROM my_table;", engine)

CREATE TEST OBJECTS(Useful for testing)
=======================================
pd.DataFrame(np.random.rand(20,5)) 				- 5 columns and 20 rows of random floats
pd.Series(my_list) 						- Creates a series from an iterable my_list
df.index = pd.date_range('1900/1/30', periods=df.shape[0]) 	- Adds a date index

VIEWING/INSPECTING DATA
=======================
df.show()				- Displaying the data
df.head(n) 				- First n rows of the DataFrame 
df.tail(n) 				- Last n rows of the DataFrame 
df.shape() 				- Number of rows and columns 
df.info() 				- Index, Datatype and Memory information
df.describe() 				- Summary statistics for numerical columns
s.value_counts(dropna=False) 		- Views unique values and counts 
df.apply(pd.Series.value_counts) 	- Unique values and counts for all columns

SELECTION
=========
df[col] 			- Returns column with label col as Series df[[col1, col2]] - Returns Columns as a new DataFrame
s.iloc[0] 			- Selection by position 
s.loc[0] 			- Selection by index 
df.iloc[0,:] 			- First row
df.iloc[0,0] 			- First element of first column

DATA CLEANING
=============
df.columns = ['a','b','c'] 			- Renames columns pd.isnull() - Checks for null Values, Returns Boolean Array
pd.notnull() 					- Opposite of s.isnull() df.dropna() - Drops all rows that contain null values
df.dropna(axis=1)				- Drops all columns that contain null values 
df.dropna(axis=1,thresh=n) 			- Drops all rows have less than n non null values 
df.fillna(x) 					- Replaces all null values with x 
s.fillna(s.mean()) 				- Replaces all null values with the mean (mean can be replaced with almost any function from the statistics section) 
s.astype(float) 				- Converts the datatype of the series to float
s.replace(1,'one') 				- Replaces all values equal to 1 with 'one' 
s.replace([1,3],['one','three']) 		- Replaces all 1 with 'one' and 3 with 'three' 
df.rename(columns=lambda x: x + 1) 		- Mass renaming of columns
df.rename(columns={'old_name': 'new_ name'}) 	- Selective renaming
df.set_index('column_one') 			- Changes the index 
df.rename(index=lambda x: x + 1) 		- Mass renaming of index

FILTER, SORT, & GROUPBY
=======================
df[df[col] > 0.5] 						- Rows where the col column is greater than 0.5
df[(df[col] > 0.5) & (df[col] < 0.7)] 				- Rows where 0.7 > col > 0.5
df.sort_values(col1) 						- Sorts values by col1 in ascending order
df.sort_values(col2,ascending=False) 				- Sorts values by col2 in descending order
df.sort_values([col1,col2], ascending=[True,False]) 		- Sorts values by
df.groupby(col) 						- Returns a groupby object for values from one column 
df.groupby([col1,col2]) 					- Returns a groupby object values from multiple columns 
df.groupby(col1)[col2].mean() 					- Returns the mean of the values in col2, grouped by the values in col1 (mean can be replaced with almost any function from the statistics section)
df.pivot_table(index=col1,values= [col2,col3],aggfunc=mean) 	- Creates a pivot table that groups by col1 and calculates the mean of col2 and col3
df.groupby(col1).agg(np.mean) 					- Finds the average across all columns for every unique column 1 group
df.apply(np.mean) 						- Applies a function across each column
df.apply(np.max, axis=1) 					- Applies a function across each row

APPLYING FUNCTIONS
==================
>>> f = lambda x: x*2
>>> df.apply(f)
>>> df.applymap(f)

JOIN/COMBINE
============
df1.append(df2) 			- Adds the rows in df1 to the end of df2 (columns should be identical)
pd.concat([df1, df2],axis=1) 		- Adds the columns in df1 to the end of df2 (rows should be identical)
df1.join(df2,on=col1,how='inner') 	- SQL-style joins the columns in df1 with the columns on df2 where the rows for col have identical values. how can be one of 'left', 'right', 'outer', 'inner'

>>> s3 = pd.Series([7, -2, 3], index=['a', 'c', 'd'])
>>> s + s3
a 10.0 b NaN c 5.0 d 7.0

STATISTICS(These can all be applied to a series as well.)
==========
df.describe() 				- Summary statistics for numerical columns
df.mean() 				- Returns the mean of all columns 
df.corr() 				- Returns the correlation between columns in a DataFrame
df.count() 				- Returns the number of non-null values in each DataFrame column 
df.max() 				- Returns the highest value in each column
df.min() 				- Returns the lowest value in each column 
df.median() 				- Returns the median of each column 
df.std() 				- Returns the standard deviation of each column

PLOTTING (matplotlib musst be installed for plotting)
========
df.plot.hist()				- Histogram for each column
df.plot.scatter(x='w',y='h')		- Scatter chart using pairs of points

========================================================== pyspark =================================================================
Initializing Spark:
>>> from pyspark import SparkContext
>>> sc = SparkContext(master = 'local[2]')

Inspect SparkContext:
====================
>>> sc.version					Retrieve SparkContext version
>>> sc.pythonVer				Retrieve Python version
>>> sc.master					Master URL to connect to
>>> str(sc.sparkHome)				Path where Spark is installed on worker nodes Retrieve name of the Spark User running SparkContext
>>> str(sc.sparkUser())				
>>> sc.appName					Return application name
>>> sc.applicationId				Retrieve application ID
>>> sc.defaultParallelism			Return default level of parallelism
>>> sc.defaultMinPartitions			Default minimum number of partitions for RDDs

Configuration:
=============
>>> from pyspark import SparkConf, SparkContext
>>> conf = (SparkConf()
	     .setMaster("local")
	     .setAppName("My app")
	     .set("spark.executor.memory", "1g"))
>>> sc = SparkContext(conf = conf)

Using the Shell:
==============
In the PySpark shell, a special interpreter-aware SparkContext is already created in the variable called sc.
$ ./bin/spark-shell --master local[2]
$ ./bin/pyspark --master local[4] --py- les code.py
Set which master the context connects to with the --master argument, and add Python .zip, .egg or .py  les to the runtime path by passing a comma-separated list to --py- les.

Loading Data:
============
Parallelized Collections
>>> rdd = sc.parallelize([('a',7),('a',2),('b',2)])
>>> rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])
>>> rdd3 = sc.parallelize(range(100))
>>> rdd4 = sc.parallelize([("a",["x","y","z"]),("b",["p", "r"])])

External Data:
Read either one text  file from HDFS, a local  file system or or any Hadoop-supported  file system URI with textFile(), or read in a directory of text  files with wholeTextFiles().

>>> textFile = sc.textFile("/my/directory/*.txt")
>>> textFile2 = sc.wholeTextFiles("/my/directory/")

Retrieving RDD Information:
==========================
Basic Information:
>>> rdd.getNumPartitions()						List the number of partitions Count RDD instances
>>> rdd.count()								Count RDD instances by key Count RDD instances by value
 3
>>> rdd.countByKey()							Return (key,value) pairs as a dictionary
defaultdict(<type 'int'>,{'a':2,'b':1}) 

>>> rdd.countByValue()
defaultdict(<type 'int'>,{('b',2):1,('a',2):1,('a',7):1}) 

>>> rdd.collectAsMap()
 {'a': 2,'b': 2}
>>> rdd3.sum()								Sum of RDD elements
 4950
>>> sc.parallelize([]).isEmpty()					Check whether RDD is empty
True

Summary:
=======
>>> rdd3.max()			# 99 # Maximum value of RDD elements 
>>> rdd3.min()			#0 #Minimum value of RDD elements 
>>> rdd3.mean()			#49.5 #Mean value of RDD elements 
>>> rdd3.stdev()		# 28.866070047722118 #Standard deviation of RDD elements 
>>> rdd3.variance()		# 833.25 #Compute variance of RDD elements 
>>> rdd3.histogram(3)		#([0,33,66,99],[33,33,34]) #Compute histogram by bins
>>> rdd3.stats()		#Summary statistics (count, mean, stdev, max & min)

Applying Functions:
==================
rdd.map(lambda x: x+(x[1],x[0])).collect()						# Apply a function to each RDD element 
>>> conf = (SparkConf()
[('a',7,7,'a'),('a',2,2,'a'),('b',2,2,'b')] 

>>> rdd5 = rdd. atMap(lambda x: x+(x[1],x[0]))
>>> rdd5.collect()
  ['a',7,7,'a','a',2,2,'a','b',2,2,'b']

>>> rdd4.flatMapValues(lambda x: x).collect()						# Apply a function to each RDD element and  flatten the result
[('a','x'),('a','y'),('a','z'),('b','p'),('b','r')]					# Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys

Selecting Data:
==============
Getting
>>> rdd.collect()
  [('a', 7), ('a', 2), ('b', 2)]
>>> rdd.take(2)
  [('a', 7), ('a', 2)]
>>> rdd.first()
  ('a', 7)
 
>>> rdd.top(2)
  [('b', 2), ('a', 7)]

Sampling:
>>> rdd3.sample(False, 0.15, 81).collect() 						# Return sampled subset of rdd3
[3,4,27,31,40,41,42,43,60,76,79,80,86,97]

Filtering:
>>> rdd.filter(lambda x: "a" in x).collect()
	[('a',7),('a',2)]
>>> rdd5.distinct().collect()
   	['a',2,'b',7]
>>> rdd.keys().collect()
	['a', 'a', 'b']

Iterating:
=========
Apply a function to all RDD elements
>>> def g(x): print(x)
>>> rdd.foreach(g)
 ('a', 7)
 ('b', 2)
 ('a', 2)

Reshaping Data:
==============
Reducing:
>>> rdd.reduceByKey(lambda x,y : x+y).collect()						# Merge the rdd values for each key
  [('a',9),('b',2)]
>>> rdd.reduce(lambda a, b: a + b)							# Merge the rdd values
  ('a',7,'a',2,'b',2)

Grouping by:
>>> rdd3.groupBy(lambda x: x % 2)							# Return RDD of grouped values
        .mapValues(list)
        .collect()
>>> rdd.groupByKey()									# Group rdd by key
       .mapValues(list)
       .collect()
[('a',[7,2]),('b',[2])]

Aggregating:
>>> seqOp = (lambda x,y: (x[0]+y,x[1]+1)) 						# Aggregate RDD elements of each partition and then the results Aggregate values of each RDD key
>>> combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) 
>>> rdd3.aggregate((0,0),seqOp,combOp)
	(4950,100)
>>> rdd.aggregateByKey((0,0),seqop,combop).collect()					# Aggregate the elements of each partition, and then the results Merge the values for each key
  	[('a',(9,2)), ('b',(2,1))]
>>> rdd3.fold(0,add)
	4950
>>> rdd.foldByKey(0, add).collect()
  	[('a',9),('b',2)]
>>> rdd3.keyBy(lambda x: x+x).collect()							# Create tuples of RDD elements by applying a function

Mathematical Operations:
=======================
>> rdd.subtract(rdd2).collect()								# Return each rdd value not contained in rdd2
  	[('b',2),('a',7)]
>>> rdd2.subtractByKey(rdd).collect()							# Return each (key,value) pair of rdd2 with no matching key in rdd
	[('d', 1)]
>>> rdd.cartesian(rdd2).collect()							# Return the Cartesian product of rdd and rdd2

Sort:
====
>>> rdd2.sortBy(lambda x: x[1]).collect()						# Sort RDD by given function
  	[('d',1),('b',1),('a',2)]
>>> rdd2.sortByKey().collect()								# Sort (key, value) RDD by key
	[('a',2),('b',1),('d',1)]

Repartitioning:
==============
>>> rdd.repartition(4)									# New RDD with 4 partitions
>>> rdd.coalesce(1)									# Decrease the number of partitions in the RDD to 1

Saving:
======
>>> rdd.saveAsTextFile("rdd.txt")
>>> rdd.saveAsHadoopFile("hdfs://namenodehost/parent/child",'org.apache.hadoop.mapred.TextOutputFormat')

>>> sc.stop()										=> Stopping SparkContext => 
$ ./bin/spark-submit examples/src/main/python/pi.py					=> Execution

========================================================== pandas elaborated with examples =========================================
https://www.geeksforgeeks.org/python-pandas-dataframe/

Python | Pandas DataFrame
Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns).
Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects.

Example 1: Creating a DataFrame using List.

import pandas as pd		# import pandas library as pd
data = ['Geeks','For','Geeks']	# creating list
df = pd.DataFrame(data)		# creating dataframe using DataFrame object

print(df)
Output:

       0
0  Geeks
1    For
2  Geeks
 

Example 2: Creating a DataFrame using Arrays

import pandas as pd
data = {'A':['Geeks', 'For', 'Geeks'],'B':['Welcome',2,'Geeks']}
df = pd.DataFrame(data, index = ['One','Two','Three'])
 
print (df)
Output:

           A        B
One    Geeks  Welcome
Two      For        2
Three  Geeks    Geeks

Python | Pandas Series
Pandas Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.). The axis labels are collectively called index.
Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index.

Example 1: Create a basic Series

import pandas as pd
ser = pd.Series()			# create empty series
print(ser)
 
ser = pd.Series([1, 2, 3, 4, 5])	# create series form a list
print(ser)
Output:

Series([], dtype: float64)

0   1
1   2
2   3
3   4
4   5
dtype: int64

Example 2: Create a Series using Dictionary

import pandas as pd
 
dict = {'Geeks' : 10,
        'for' : 20,
        'geeks' : 30}
 
ser = pd.Series(dict)
 
print(ser)
Output:

Geeks    10
for      20
geeks    30
dtype: int64

Python | Pandas Working With Text Data
Series and Indexes are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods.

Example 1: Use of Str.lower() method

import pandas as pd 
   
data = pd.read_csv("employees.csv") 			# making data frame from csv file 
data["First Name"]= data["First Name"].str.lower() 	# converting and overwriting values in column 
data 							# display 
 
Example 2 : Use of str.find() method

import pandas as pd 
   
data = pd.read_csv("https://cdncontribute.geeksforgeeks.org/wp-content/uploads/nba.csv") 					# reading csv file from url
data.dropna(inplace = True) 													# dropping null value columns to avoid errors
sub ='a'															# substring to be searched 
   
# creating and passsing series to new column 
data["Indexes"]= data["Name"].str.find(sub) 
   
# display 
data 

Python | Pandas Working with Dates and Times
Pandas has proven very successful as a tool for working with time series data, especially in the financial data analysis space. Using the NumPy datetime64 and timedelta64 dtypes, we have consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data.

Example #1: Create a dates dataframe

import pandas as pd
 
# Create dates dataframe with frequency  
data = pd.date_range('1/1/2011', periods = 10, freq ='H')
 
data
Output:
DatetimeIndex(['2011-01-01 00:00:00', '2011-01-01 01:00:00',
               '2011-01-01 02:00:00', '2011-01-01 03:00:00',
               '2011-01-01 04:00:00', '2011-01-01 05:00:00',
               '2011-01-01 06:00:00', '2011-01-01 07:00:00',
               '2011-01-01 08:00:00', '2011-01-01 09:00:00'],
              dtype='datetime64[ns]', freq='H')


Example #2: Create range of dates and show basic features

# Create date and time with dataframe
data = pd.date_range('1/1/2011', periods = 10, freq ='H')
 
x = datetime.now()
x.month, x.year
Output:

(9, 2018)
 
Example #3: Break data and time into seperate features

# Create date and time with dataframe
rng = pd.DataFrame()
rng['date'] = pd.date_range('1/1/2011', periods = 72, freq ='H')
 
# Print the dates in dd-mm-yy format
rng[:5]
 
# Create features for year, month, day, hour, and minute
rng['year'] = rng['date'].dt.year
rng['month'] = rng['date'].dt.month
rng['day'] = rng['date'].dt.day
rng['hour'] = rng['date'].dt.hour
rng['minute'] = rng['date'].dt.minute
 
# Print the dates divided into features
rng.head(3)
Output:

Python | Pandas Merging, Joining, and Concatenating
Pandas provide various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.

Merge, Join and Concatenate DataFrames using Panda

Example #1 : DataFrames Concatenation

concat() function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.

# Python program to concatenate
# dataframes using Panda
 
# Creating first dataframe
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3'],
                    'C': ['C0', 'C1', 'C2', 'C3'],
                    'D': ['D0', 'D1', 'D2', 'D3']},
                    index = [0, 1, 2, 3])
 
# Creating second dataframe
df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
                    'B': ['B4', 'B5', 'B6', 'B7'],
                    'C': ['C4', 'C5', 'C6', 'C7'],
                    'D': ['D4', 'D5', 'D6', 'D7']},
                    index = [4, 5, 6, 7])
 
# Creating third dataframe
df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],
                    'B': ['B8', 'B9', 'B10', 'B11'],
                    'C': ['C8', 'C9', 'C10', 'C11'],
                    'D': ['D8', 'D9', 'D10', 'D11']},
                    index = [8, 9, 10, 11])
 
# Concatenating the dataframes
pd.concat([df1, df2, df3])
Output:
Concatenation

 
Example #2 : DataFrames Merge

Pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame objects.

# Python program to merge
# dataframes using Panda
 
# Dataframe created
left = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'],
                    'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']})
 
right = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'],
                      'C': ['C0', 'C1', 'C2', 'C3'],
                      'D': ['D0', 'D1', 'D2', 'D3']})
                       
# Merging the dataframes                      
pd.merge(left, right, how ='inner', on ='Key')
Output:
Merging
 
Code #3 : DataFrames Join

# Python program to join
# dataframes using Panda
 
left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']},
                    index = ['K0', 'K1', 'K2', 'K3'])
 
right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],
                      'D': ['D0', 'D1', 'D2', 'D3']},
                      index = ['K0', 'K1', 'K2', 'K3'])
                       
# Joining the dataframes                      
left.join(right)
Output:

Joining

Python | Data analysis using Pandas

Pandas is the most popular python library that is used for data analysis. It provides highly optimized performance with back-end source code is purely written in C or Python.

We can analyze data in pandas with:

Series
DataFrames
Series:

Series is one dimensional(1-D) array defined in pandas that can be used to store any data type.

Code #1: Creating Series

filter_none
edit
play_arrow
brightness_4
# Program to create series 
import pandas as pd  # Import Panda Library 
  
# Create series with Data, and Index 
a = pd.Series(Data, index = Index)   
Here, Data can be:

A Scalar value which can be integerValue, string
A Python Dictionary which can be Key, Value pair
A Ndarray
Note: Index by default is from 0, 1, 2, …(n-1) where n is length of data.
 
Code #2: When Data contains scalar values



 

filter_none
edit
play_arrow
brightness_4
# Program to Create series with scalar values  
Data =[1, 3, 4, 5, 6, 2, 9]  # Numeric data 
  
# Creating series with default index values 
s = pd.Series(Data)     
  
# predefined index values 
Index =['a', 'b', 'c', 'd', 'e', 'f', 'g']  
  
# Creating series with predefined index values 
si = pd.Series(Data, Index)  
Output:
>>> print si
a    1
b    3
c    4
d    5
e    6
f    2
g    9

Scalar Data with default Index



Scalar Data with Index


 
Code #3: When Data contains Dictionary

filter_none
edit
play_arrow
brightness_4
# Program to Create Dictionary series 
dictionary ={'a':1, 'b':2, 'c':3, 'd':4, 'e':5}  
  
# Creating series of Dictionary type 
sd = pd.Series(dictionary)  
Output:


Dictionary type data


 

Code #4:When Data contains Ndarray

filter_none
edit
play_arrow
brightness_4
# Program to Create ndarray series 
Data =[[2, 3, 4], [5, 6, 7]]  # Defining 2darray 
  
# Creating series of 2darray 
snd = pd.Series(Data)     
Output:


Data as Ndarray


 

DataFrames:

DataFrames is two-dimensional(2-D) data structure defined in pandas which consists of rows and columns.

Code #1: Creation of DataFrame

filter_none
edit
play_arrow
brightness_4
# Program to Create DataFrame 
import pandas as pd   # Import Library 
  
a = pd.DataFrame(Data)  # Create DataFrame with Data 
Here, Data can be:



 

One or more dictionaries
One or more Series
2D-numpy Ndarray
 
Code #2: When Data is Dictionaries

filter_none
edit
play_arrow
brightness_4
# Program to Create Data Frame with two dictionaries 
dict1 ={'a':1, 'b':2, 'c':3, 'd':4}        # Define Dictionary 1 
dict2 ={'a':5, 'b':6, 'c':7, 'd':8, 'e':9} # Define Dictionary 2 
Data = {'first':dict1, 'second':dict2}  # Define Data with dict1 and dict2 
df = pd.DataFrame(Data)  # Create DataFrame 
Output:


DataFrame with two dictionaries


 
Code #3: When Data is Series

filter_none
edit
play_arrow
brightness_4
# Program to create Dataframe of three series  
import pandas as pd 
  
s1 = pd.Series([1, 3, 4, 5, 6, 2, 9])           # Define series 1 
s2 = pd.Series([1.1, 3.5, 4.7, 5.8, 2.9, 9.3]) # Define series 2 
s3 = pd.Series(['a', 'b', 'c', 'd', 'e'])     # Define series 3 
  
  
Data ={'first':s1, 'second':s2, 'third':s3} # Define Data 
dfseries = pd.DataFrame(Data)              # Create DataFrame 
Output:


DataFrame with three series


 
Code #4: When Data is 2D-numpy ndarray
Note: One constraint has to be maintained while creating DataFrame of 2D arrays – Dimensions of 2D array must be same.

filter_none
edit
play_arrow
brightness_4
# Program to create DataFrame from 2D array 
import pandas as pd # Import Library 
d1 =[[2, 3, 4], [5, 6, 7]] # Define 2d array 1 
d2 =[[2, 4, 8], [1, 3, 9]] # Define 2d array 2 
Data ={'first': d1, 'second': d2} # Define Data  
df2d = pd.DataFrame(Data)    # Create DataFrame 
Output:


DataFrame with 2d ndarray
https://www.geeksforgeeks.org/python-data-analysis-using-pandas/
Python | Read csv using pandas.read_csv()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Import Pandas:

import pandas as pd
 
Code #1 : read_csv is an important pandas function to read csv files and do operations on it.

filter_none
brightness_4
# Import pandas 
import pandas as pd 
  
# reading csv file  
pd.read_csv("filename.csv") 
Opening a CSV file through this is easy. But there are many others thing one can do through this function only to change the returned object completely. For instance, one can read a csv file not only locally, but from a URL through read_csv or one can choose what columns needed to export so that we don’t have to edit the array later.

Here is the list of parameters it takes with their Default values.



 

pd.read_csv(filepath_or_buffer, sep=’, ‘, delimiter=None, header=’infer’, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=’infer’, thousands=None, decimal=b’.’, lineterminator=None, quotechar='”‘, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)
Not all of them are much important but remembering these actually save time of performing same functions on own. One can see parameters of any function by pressing shift + tab in jupyter notebook. Useful ones are given below with their usage :

PARAMETER	USE
filepath_or_buffer	URL or Dir location of file
sep	Stands for seperator, default is ‘, ‘ as in csv(comma seperated values)
index_col	Makes passed column as index instead of 0, 1, 2, 3…r

header	Makes passed row/s[int/int list] as header


use_cols	Only uses the passed col[string list] to make data frame
squeeze	If true and only one column is passed, returns pandas series
skiprows	Skips passed rows in new data frame
Refer the link to data set used from here.

Code #2 :

filter_none
brightness_4
# importing Pandas library 
import pandas as pd 
  
pd.read_csv(filepath_or_buffer = "pokemon.csv") 
  
# makes the passed rows header 
pd.read_csv("pokemon.csv", header =[1, 2]) 
  
# make the passed column as index instead of 0, 1, 2, 3.... 
pd.read_csv("pokemon.csv", index_col ='Type') 
  
# uses passed cols only for data frame 
pd.read_csv("pokemon.csv", usecols =["Type"]) 
  
# reutruns pandas series if there is only one colunmn 
pd.read_csv("pokemon.csv", usecols =["Type"], 
                              squeeze = True) 
                                
# skips the passed rows in new series 
pd.read_csv("pokemon.csv", 
            skiprows = [1, 2, 3, 4]) 

Python | Merge, Join and Concatenate DataFrames using Panda

A dataframe is a two-dimensional data structure having multiple rows and columns. In a dataframe, the data is aligned in the form of rows and columns only. A dataframe can perform arithmetic as well as conditional operations. It has mutable size.

Below is the implementation using Numpy and Pandas.

Modules needed:

import numpy as np
import pandas as pd
 

Code #1 : DataFrames Concatenation
concat() function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.

 

filter_none
brightness_4
# Python program to concatenate 
# dataframes using Panda 
  
# Creating first dataframe 
df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3'], 
                    'C': ['C0', 'C1', 'C2', 'C3'], 
                    'D': ['D0', 'D1', 'D2', 'D3']}, 
                    index = [0, 1, 2, 3]) 
  
# Creating second dataframe 
df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], 
                    'B': ['B4', 'B5', 'B6', 'B7'], 
                    'C': ['C4', 'C5', 'C6', 'C7'], 
                    'D': ['D4', 'D5', 'D6', 'D7']}, 
                    index = [4, 5, 6, 7]) 
  
# Creating third dataframe 
df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'], 
                    'B': ['B8', 'B9', 'B10', 'B11'], 
                    'C': ['C8', 'C9', 'C10', 'C11'], 
                    'D': ['D8', 'D9', 'D10', 'D11']}, 
                    index = [8, 9, 10, 11]) 
  
# Concatenating the dataframes 
pd.concat([df1, df2, df3]) 
Output:
Concatenation

 
Code #2 : DataFrames Merge
Pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame objects.

filter_none
brightness_4
# Python program to merge 
# dataframes using Panda 
  
# Dataframe created 
left = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'], 
                    'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3']}) 
  
right = pd.DataFrame({'Key': ['K0', 'K1', 'K2', 'K3'], 
                      'C': ['C0', 'C1', 'C2', 'C3'], 
                      'D': ['D0', 'D1', 'D2', 'D3']}) 
                        
# Merging the dataframes                       
pd.merge(left, right, how ='inner', on ='Key') 
Output:
Merging
 
Code #3 : DataFrames Join

filter_none
brightness_4
# Python program to join 
# dataframes using Panda 
  
left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 
                    'B': ['B0', 'B1', 'B2', 'B3']}, 
                    index = ['K0', 'K1', 'K2', 'K3']) 
  
right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'], 
                      'D': ['D0', 'D1', 'D2', 'D3']}, 
                      index = ['K0', 'K1', 'K2', 'K3']) 
                        
# Joining the dataframes                       
left.join(right) 
Output:

Joining

Python | Delete rows/columns from DataFrame using Pandas.drop()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Pandas provide data analysts a way to delete and filter data frame using .drop() method. Rows or columns can be removed using index label or column name using this method.

Syntax:
DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’)

Parameters:

labels: String or list of strings referring row or column name.
axis: int or string value, 0 ‘index’ for Rows and 1 ‘columns’ for Columns.
index or columns: Single label or list. index or columns are an alternative to axis and cannot be used together.
level: Used to specify level in case data frame is having multiple level index.
inplace: Makes changes in original Data Frame if True.
errors: Ignores error if any value from the list doesn’t exists and drops rest of the values when errors = ‘ignore’



 

Return type: Dataframe with dropped values
To download the CSV used in code, click here.
Example #1: Dropping Rows by index label
In his code, A list of index labels is passed and the rows corresponding to those labels are dropped using .drop() method.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed values 
data.drop(["Avery Bradley", "John Holland", "R.J. Hunter", 
                            "R.J. Hunter"], inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed values. Those values were dropped and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping values-


Data Frame after Dropping values-


 

Example #2 : Dropping columns with column name

In his code, Passed columns are dropped using column names. axis parameter is kept 1 since 1 refers to columns.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed columns 
data.drop(["Team", "Weight"], axis = 1, inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed columns. Those values were dropped since axis was set equal to 1 and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping Columns-


Data Frame after Dropping Columns-

Python | Delete rows/columns from DataFrame using Pandas.drop()

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages and makes importing and analyzing data much easier.

Pandas provide data analysts a way to delete and filter data frame using .drop() method. Rows or columns can be removed using index label or column name using this method.

Syntax:
DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=’raise’)

Parameters:

labels: String or list of strings referring row or column name.
axis: int or string value, 0 ‘index’ for Rows and 1 ‘columns’ for Columns.
index or columns: Single label or list. index or columns are an alternative to axis and cannot be used together.
level: Used to specify level in case data frame is having multiple level index.
inplace: Makes changes in original Data Frame if True.
errors: Ignores error if any value from the list doesn’t exists and drops rest of the values when errors = ‘ignore’



 

Return type: Dataframe with dropped values
To download the CSV used in code, click here.
Example #1: Dropping Rows by index label
In his code, A list of index labels is passed and the rows corresponding to those labels are dropped using .drop() method.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed values 
data.drop(["Avery Bradley", "John Holland", "R.J. Hunter", 
                            "R.J. Hunter"], inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed values. Those values were dropped and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping values-


Data Frame after Dropping values-


 

Example #2 : Dropping columns with column name

In his code, Passed columns are dropped using column names. axis parameter is kept 1 since 1 refers to columns.

filter_none
brightness_4
# importing pandas module 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("nba.csv", index_col ="Name" ) 
  
# dropping passed columns 
data.drop(["Team", "Weight"], axis = 1, inplace = True) 
  
# display 
data 
Output:
As shown in the output images, the new output doesn’t have the passed columns. Those values were dropped since axis was set equal to 1 and the changes were made in the original data frame since inplace was True.

Data Frame before Dropping Columns-


Data Frame after Dropping Columns-

Python | Data Comparison and Selection in Pandas

Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages, and makes importing and analyzing data much easier.

The most important thing in Data Analysis is comparing values and selecting data accordingly. The “==” operator works for multiple values in a Pandas Data frame too. Following two examples will show how to compare and select data from a Pandas Data frame.

To download the CSV file used, Click Here.

Example #1: Comparing Data
In the following example, a data frame is made from a csv file. In the Gender Column, there are only 3 types of values (“Male”, “Female” or NaN). Every row of Gender column is compared to “Male” and a boolean series is returned after that.

filter_none
brightness_4
# importing pandas package 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("employees.csv") 
  
# storing boolean series in new 
new = data["Gender"] == "Male"
  
# inserting new series in data frame 
data["New"]= new 
  
# display 
data 
Output:
As show in the output image, for Gender= “Male”, the value in New Column is True and for “Female” and NaN values it is False.



 


 
Example #2: Selecting Data
In the following example, the boolean series is passed to the data and only Rows having Gender=”Male” are returned.

filter_none
brightness_4
# importing pandas package 
import pandas as pd 
  
# making data frame from csv file 
data = pd.read_csv("employees.csv") 
  
# storing boolean series in new 
new = data["Gender"] != "Female"
  
# inserting new series in data frame 
data["New"]= new 
  
# display 
data[new] 
  
# OR  
# data[data["Gender"]=="Male"] 
# Both are the same 
Output:
As shown in the output image, Data frame having Gender=”Male” is returned.



pandas.pydata			(This is vast; needs to be learned separately)
=============
df.expanding()			- Return an Expanding object allowing summary functions to be applied cumulatively.
df.rolling(n)			- Return a Rolling object allowing summary functions to be applied to windows of length n.

==================================== Apache Spark : RDD vs DataFrame vs Dataset ===========================================================

With Spark2.0 release, there are 3 types of data abstractions which Spark officially provides now to use : RDD,DataFrame and DataSet


Short Combined Intro :
Evolution of these abstractions happened in this way :

RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6)					#RDD being the oldest available from 1.0 version to Dataset being the newest available from 1.6 version.

Given same data, each of the 3 abstraction will compute and give same results to user. But they differ in performance and the ways they compute.

RDD lets us decide HOW we want to do which limits the optimisation Spark can do on processing underneath where as dataframe/dataset lets us decide WHAT we want to do and leave everything on Spark to decide how to do computation.

We will understand this in 2 minutes what is meant by HOW & WHAT

Dataframe came as a major performance improvement over RDD but not without some downsides.

This led to development of Dataset which is an effort to unify best of RDD and data frame.

In future, Dataset will eventually replace RDD and Dataframe to become the only API spark users should be using in code.

Lets understand them in detail one by one.

RDD:
-Its building block of spark. No matter which abstraction Dataframe or Dataset we use, internally final computation is done on RDDs.
-RDD is lazily evaluated immutable parallel collection of objects exposed with lambda functions.
-The best part about RDD is that it is simple. It provides familiar OOPs style APIs with compile time safety. We can load any data from a source,convert them into RDD and store in memory to compute results. RDD can be easily cached 
if same set of data needs to recomputed.

-But the disadvantage is performance limitations. Being in-memory jvm objects, RDDs involve overhead of Garbage Collection and Java(or little better Kryo) Serialisation which are expensive when data grows.

Dataframe:
-DataFrame is an abstraction which gives a schema view of data. Which means it gives us a view of data as columns with column name and types info, We can think data in data frame like a table in database.
-Like RDD, execution in Dataframe too is lazy triggered .
-offers huge performance improvement over RDDs because of 2 powerful features it has:
	1. Custom Memory management (aka Project Tungsten)
		Data is stored in off-heap memory in binary format. This saves a lot of memory space. Also there is no Garbage Collection overhead involved. By knowing the schema of data in advance and storing efficiently in 
		binary format, expensive java Serialization is also avoided.

	2. Optimized Execution Plans (aka Catalyst Optimizer)
		Query plans are created for execution using Spark catalyst optimiser. After an optimised execution plan is prepared going through some steps, the final execution happens internally on RDDs only but thats completely 
		hidden from the users.

Dataframe Limitations:-
Compile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not known. However, you will get a Runtime exception when executing this code.
Example:

case class Person(name : String , age : Int) 
val dataframe = sqlContect.read.json("people.json") 
dataframe.filter("salary > 10000").show 
=> throws Exception : cannot resolve 'salary' given input age , name
This is challenging specially when you are working with several transformation and aggregation steps.

Cannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be 
recover the original RDD of Person class (RDD[Person]).
Example:

case class Person(name : String , age : Int)
val personRDD = sc.makeRDD(Seq(Person("A",10),Person("B",20)))
val personDF = sqlContext.createDataframe(personRDD)
personDF.rdd // returns RDD[Row] , does not returns RDD[Person]

Dataframe example:
2 ways to define: 1. Expression BuilderStyle 2. SQL Style

As discussed, If we try using some columns not present in schema, we will get problem only at runtime . For example, if we try accessing salary when only name and age are present in the schema will throw AnalysisException


Dataset Features:-
Provides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)
Encoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.
Programming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.
Type Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.
-It is an extension to Dataframe API, the latest abstraction which tries to provide best of both RDD and Dataframe.

-comes with OOPs style and developer friendly compile time safety like RDD as well as performance boosting features of Dataframe : Catalyst optimiser and custom memory management.
-How dataset scores over Dataframe is an additional feature it has: Encoders .
-Encoders act as interface between JVM objects and off-heap custom memory binary format data.

-Encoders generate byte code to interact with off-heap data and provide on-demand access to individual attributes without having to de-serialize an entire object.

-case class is used to define the structure of data schema in Dataset. Using case class, its very easy to work with dataset. Names of different attributes in case class is directly mapped to attributes in Dataset . It gives feeling 
like working with RDD but actually underneath it works same as Dataframe.

Dataframe is infact treated as dataset of generic row objects. DataFrame=Dataset[Row] . So we can always convert a data frame at any point of time into a dataset by calling ‘as’ method on Dataframe.
 e.g. http://df.as[MyClass]

Important point to remember is that both Dataset and DataFrame internally does final execution on RDD objects only but the difference is users do not write code to create the RDD collections and have no control as such over RDDs. RDDs are created in the execution plan as last stage after deciding and going through all the optimizations (see Execution Plan Diagram).

And all these optimisations could have been possible because data is structured and Spark knows about the schema of data in advance. So it can apply all the powerful features like tungsten custom memory off-heap binary storage,catalyst 
optimiser and encoders to get the performance which was not possible if users would have been directly working on RDD.

Conclusion:

In short, Spark is moving from unstructured computation(RDDs) towards structured computation because of many performance optimisations it allows . Data frame was a step in direction of structured computation but lacked developer friendliness of compile time safety,lambda functions. Finally Dataset is the unification of Dataframe and RDD to bring the best abstraction out of two.

Going forward developers should only be concerned about DataSet while Dataframe and RDD will be discouraged to use. But its always better to be aware of the legacy for better understanding of internals.

Interestingly,most of these new concepts like custom memory management(tungsten),logical/physical plans(catalyst optimizer),encoders(dataset),etc seems to be inspired from its competitor Apache Flink which inherently supports these since inception.There are other new powerful feature enhancements like windowing,sessions,etc coming in Spark which are already in Flink. So its better to keep a close watch on both Spark and Flink in coming days.

==================================== Sparkx.py  ==============================================================

from common_base.base_test import ReactorXTestBase
import os, sys, json

class SparkX(ReactorXTestBase):

    # Constructor
    def __init__(self):

        batchx_path = self.get_value('batchx_path')

        SPARK_HOME = os.path.join(batchx_path, 'app/spark-2.1.1-bin-hadoop2.6/')
        self.assertTrue(
            os.path.exists(SPARK_HOME),
            'Make sure that SPARK_HOME[{}] exists and is a valid path.'.format(SPARK_HOME)
        )

        # Add the PySpark directories to the Python path:
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python'))
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python', 'pyspark'))
        sys.path.insert(1, os.path.join(SPARK_HOME, 'python', 'build'))

        # This is to put a check if the version of py4j changes in future
        py4j_src_path = os.path.join(SPARK_HOME, 'python', 'lib/py4j-0.10.4-src.zip')
        self.assertTrue(
            os.path.exists(py4j_src_path),
            'Make sure that py4j_src_path[{}] exists and is a valid path.'.format(py4j_src_path)
        )

        sys.path.insert(1, py4j_src_path)
        # print SPARK_HOME

        from pyspark.conf import SparkConf
        from pyspark.sql import SparkSession, SQLContext, Row
        from pyspark.sql.types import StringType, IntegerType, StructType, StructField

        """Create a single node Spark application."""
        sc_conf = SparkConf()
        sc_conf.setAppName("batchx_tests")
        sc_conf.setMaster('local[2]')
        sc_conf.set('spark.executor.memory', '2g')
        sc_conf.set('spark.executor.cores', '4')
        sc_conf.set('spark.cores.max', '40')
        sc_conf.set('spark.logConf', True)
        # print sc_conf.getAll()

        SparkSession._instantiatedContext = None
        self.spark = SparkSession.builder.config(conf=sc_conf).getOrCreate()
        self.sparkSession = SparkSession

        sc = None
        try:
            sc.stop()
            sc = self.spark.sparkContext
        except:
            sc = self.spark.sparkContext

        sc.setLogLevel("ERROR")

        self.sqlContext = SQLContext(sc)
        self.row = Row
        self.sc = sc
        self.stringtype = StringType
        self.structtype = StructType
        self.structfield = StructField

    def convert_data_to_dataframe(self, data, table_name='data_table'):
        '''
        Converts a list or any other data type to dataframe
        :param data:
        :param table_name:
        :return:
        '''
        schematype = None
        if type(data) is str:
            schematype = self.stringtype

        # Update the method is you know the schematype for list or dictionary or else keep it as None.
        # e.g. if type(data) is list: . . .

        df = self.spark.createDataFrame(data, schematype)
        df.createOrReplaceTempView(table_name)
        return df

    def readDBF(self, dbf_file):
        '''
        read .dbf file
        :param dbf_file:
        :return:
        '''
        from dbfread import DBF
        # print len(DBF(dbf_file))
        # if i use "load = true" to load the file first, the search is faster, but it takes a lot of memory.
        return DBF(dbf_file, load=True)

        """for record in DBF(dbf_file, load=True):
            print record
            for key, value in record.items():
                try:
                    value = str(value)
                except:
                    value = value.encode('utf-8')"""

    def convert_shape_file_to_geojson(self, shp_path):
        '''
        Convert shape file to geojson
        :param shp_path:
        :return:
        '''

        import io
        import shapefile

        blocks = self.sc.binaryFiles(shp_path)
        block_dict = dict(blocks.collect())

        reader = shapefile.Reader(
            shp=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".shp")][0]]),
            shx=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".shx")][0]]),
            dbf=io.BytesIO(block_dict[[i for i in block_dict.keys() if i.endswith(".dbf")][0]])
        )

        field_names = [x[0] for x in reader.fields][1:]
        buffer = []
        for sr in reader.shapeRecords():
            atr = dict(zip(field_names, sr.record))
            geom = sr.shape.__geo_interface__
            buffer.append(
                dict(type="Feature",
                     geometry=geom,
                     properties=atr
                     )
            )

        data = json.dumps({"type": "FeatureCollection", "features": buffer}, indent=2)
        self.sc.parallelize(data)

        batchx_path = self.get_value('batchx_path')
        with open(os.path.join(batchx_path, 'urban_area.geojson'), "w") as geojson:
            geojson.write(str(data))

        '''
        # As the data is too large currently writing it to a geojson file and reading it later.
        # I'll find out a better way of implementing this
        with open(os.path.join(batchx_path, 'urban_area.geojson'), "w") as geojson:
            geojson.write(data)'''

    def read_suggested_geojson(self, geojson_file='data/outputUrbanRural/suggestedGrid.geojson'):
        '''
        read suggested geojson
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        file_path = os.path.join(batchx_path, geojson_file)
        return self.read_json_file_return_rdd(file_path)

    def read_json_file_return_rdd(self, file_path):
        '''
        read json return rdd
        :param file_path:
        :return:
        '''
        data = None
        with open(file_path, 'r') as fh:
            data = fh.read()
        json_data = json.loads(data)
        all_coordinates = []
        for feature in json_data['features']:
            for coordinates in feature['geometry']['coordinates'][0]:
                all_coordinates.append(coordinates)

        rdd = self.sc.parallelize(all_coordinates)
        return rdd

    def validate_todo_filter_type_existence(self, todo_type_filter, rel_todo_file_path):
        '''
        validate todo filter type exists or not
        :param todo_type_filter:
        :param rel_todo_file_path:
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        todo_file_path = os.path.join(batchx_path, rel_todo_file_path)

        table_name = self.get_todo_items_rdd_table(todo_file_path)

        data = self.sqlContext.sql(
            "SELECT todo_id FROM " + table_name + " WHERE item.type == \'" + todo_type_filter + "\'"
        )
        '''For testing'''
        # data.show()
        # data = self.sqlContext.sql("SELECT todo_id FROM " + table_name + " WHERE item.type == 'MANEUVER_RESTRICTION'")

        todo_ids = data.rdd.map(lambda x: str(x[0])).collect()
        self.assertTrue(
            len(todo_ids) == 0,
            'Failed to apply filter type[{}] for todo_ids [{}]'.format(todo_type_filter, str(todo_ids))
        )

    def get_todo_items_rdd_table(self, todo_file_path):
        '''
        :param todo_file_path:
        :return:
        '''
        df = self.sqlContext.read.json(todo_file_path)
        # for testing
        # print df.first().__getitem__('item').__getitem__('type')
        # df.select('item').show()
        # df.select('item').select('type').show()
        # df.printSchema()
        table_name = "todo_table"
        df.createOrReplaceTempView(table_name)
        return table_name

    def convert_rdd_to_dataframe(self, user_rdd, field_name):
        '''
        :param user_rdd: rdd
        :param field_name: feature_id
        :return:
        '''
        schema = self.structtype([self.structfield(field_name, self.stringtype(), True)])
        user_df = self.sqlContext.createDataFrame(user_rdd.map(lambda x: (x,)), schema)
        return user_df.distinct()

    def get_dataframe(self, khRepo='data/khRepo/', table_name='khRepo_table'):
        '''
        Read data/khRepo and return dataframe
        :param khRepo:
        :param table_name:
        :return:
        '''
        batchx_path = self.get_value('batchx_path')
        dataFrame = self.spark.read.json(os.path.join(batchx_path, khRepo))  # time taken =  10.959002018
        dataFrame.createOrReplaceTempView(table_name)  # Create a temporary view using the DataFrame
        return dataFrame

    # Destructor
    def __del__(self):
        try:
            self.sc.stop()
        except:
            self.logger.info('Spark context is not set yet.')

================================================================= Calling function ==================================
        sparkx = SparkX()

        df = sparkx.get_dataframe('data/khRepo/')
        self.logger.info(
            "{}: input features count: {}. get downloaded features".format(
                test_name, df.count()
            )
        )
        df.show()

        df_tool_output_featureproto = sparkx.get_dataframe('data/output/tool_output.featureproto')
        self.logger.info(
            "{}: output features count: {}. Reading tool_output.featureproto".format(
                test_name, df_tool_output_featureproto.count()
            )
        )

        df_tool_output_featureproto.show()  # just to print on console

        df_actual_features = df.join(
            df_tool_output_featureproto,
            df.feature_id == df_tool_output_featureproto.feature_id
        ).select(
            df_tool_output_featureproto.FeatureProto
        ).where(df.FeatureProto.feature_type[0] == 'ROAD_SEGMENT')

        df_actual_features.show()
============================================================== Calling function 2 =====================================
        sparkx = SparkX()

        # Make sure that expected number of buildings feature ids are downloaded in building-footprints.json
        df_building_footprints = sparkx.get_dataframe('data/buildingFiles/building-footprints/building-footprints.json')
        actual_building_footprints = df_building_footprints.count()

        # Make sure that expected number of 3DV building feature ids are converted to NDM
        df_tool_output = sparkx.get_dataframe('data/output/tool_output.featureproto')
        tool_output_feature_count = df_tool_output.count()

        # Make sure that all the 3DV building footprints are converted to NDM
        # After conversion to NDM features should have build_id starting with string "NeutronBatchX /  reactorx-apps-"
        three_dv_to_ndm_features = df_building_footprints.join(
            df_tool_output
        ).select(
            df_tool_output.metadata.source_attribution.sub_path_source[0].process_info.build_id,
            # Fields below have unique values i.e. the values are not same 3DV buildings and NDM featureProto
            df_tool_output.FeatureProto.is_active,  # is_active is not in buildin_footprints
            df_tool_output.FeatureProto.protocol_version,
            df_building_footprints.FeatureProto.protocol_version,
            df_tool_output.FeatureProto.version,
            df_building_footprints.FeatureProto.version,
            df_tool_output.FeatureProto.representative_point,
            df_building_footprints.FeatureProto.representative_point
        ).withColumnRenamed(  # Rename the column as it's too big
            'metadata.source_attribution.sub_path_source[0].process_info.build_id', 'build_id'
        ).where(  # Verifying if there is any mismatch in any of the below fields
            (df_tool_output.FeatureProto.feature_type[0] == 'BUILDING') &
            (df_tool_output.FeatureProto.feature_id == df_building_footprints.FeatureProto.feature_id) &
            (df_tool_output.FeatureProto.building.roof_type == df_building_footprints.FeatureProto.building.roof_type) &
            (
                    df_tool_output.FeatureProto.building.base_height == df_building_footprints.FeatureProto.building.base_height) &
            (
                    df_tool_output.FeatureProto.building.top_height == df_building_footprints.FeatureProto.building.top_height) &
            (df_tool_output.FeatureProto.iso_country_code == df_building_footprints.FeatureProto.iso_country_code) &
            (
                    df_tool_output.FeatureProto.iso_subdivision_code == df_building_footprints.FeatureProto.iso_subdivision_code) &
            (df_tool_output.FeatureProto.history[0].edit_type == df_building_footprints.FeatureProto.history[
                0].edit_type) &
            (df_tool_output.FeatureProto.history[0].source == df_building_footprints.FeatureProto.history[0].source) &
            (df_tool_output.FeatureProto.history[0].source_id == df_building_footprints.FeatureProto.history[
                0].source_id) &
            (df_tool_output.FeatureProto.history[0].timestamp == df_building_footprints.FeatureProto.history[
                0].timestamp) &
            (df_tool_output.FeatureProto.history[0].vendor_id == df_building_footprints.FeatureProto.history[
                0].vendor_id)
        ).where(
            "build_id like '%NeutronBatchX /  reactorx-apps-%'"
        )
        three_dv_to_ndm_features.show()  # Just for reference. It prints the table

        three_dv_to_ndm_features_count = three_dv_to_ndm_features.count()
